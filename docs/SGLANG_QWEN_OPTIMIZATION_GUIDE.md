# SGLang + Qwen2.5-3B-Instruct: Guia de Configura√ß√£o e Otimiza√ß√£o

Este documento analisa as configura√ß√µes do servidor SGLang com o modelo Qwen/Qwen2.5-3B-Instruct e fornece recomenda√ß√µes de otimiza√ß√£o para o Profile Builder.

---

## üìã √çndice

1. [Configura√ß√£o Atual do Servidor](#1-configura√ß√£o-atual-do-servidor)
2. [Par√¢metros Cr√≠ticos para Performance](#2-par√¢metros-cr√≠ticos-para-performance)
3. [Par√¢metros de Structured Output (XGrammar)](#3-par√¢metros-de-structured-output-xgrammar)
4. [Par√¢metros de Mem√≥ria e Batching](#4-par√¢metros-de-mem√≥ria-e-batching)
5. [Configura√ß√£o do Profile Builder](#5-configura√ß√£o-do-profile-builder)
6. [Recomenda√ß√µes de Otimiza√ß√£o](#6-recomenda√ß√µes-de-otimiza√ß√£o)
7. [Troubleshooting](#7-troubleshooting)

---

## 1. Configura√ß√£o Atual do Servidor

### ServerArgs Completo (Extra√≠do dos Logs)

```
model_path='Qwen/Qwen2.5-3B-Instruct'
context_length=32768
chunked_prefill_size=8192
max_prefill_tokens=16384
grammar_backend='xgrammar'
attention_backend='flashinfer'
sampling_backend='flashinfer'
mem_fraction_static=0.836
dtype='auto'
port=8000
```

---

## 2. Par√¢metros Cr√≠ticos para Performance

### 2.1 `context_length` (32768)

| Aspecto | Descri√ß√£o |
|---------|-----------|
| **O que √©** | Tamanho m√°ximo da janela de contexto do modelo |
| **Valor atual** | 32.768 tokens |
| **Por que importa** | Define o tamanho m√°ximo de entrada + sa√≠da combinados |
| **Impacto** | ‚Üë permite entradas maiores; ‚Üì economiza mem√≥ria |

**Recomenda√ß√£o para Profile Builder:**
- Manter em 32768 (padr√£o do Qwen2.5-3B)
- Nosso `max_chunk_tokens` (12000) + `max_output_tokens` (4096) = 16k, bem abaixo do limite

### 2.2 `chunked_prefill_size` (8192)

| Aspecto | Descri√ß√£o |
|---------|-----------|
| **O que √©** | Tamanho m√°ximo de tokens processados em cada chunk durante prefill |
| **Valor atual** | 8192 tokens |
| **Por que importa** | Controla uso de mem√≥ria durante gera√ß√£o de KV cache |
| **Trade-off** | ‚Üë maior throughput para prompts longos; ‚Üì menor pico de mem√≥ria |

**Recomenda√ß√£o:**
```bash
# Para GPU com 24GB (RTX 4090/A10):
--chunked-prefill-size 4096

# Para GPU com 16GB ou menos:
--chunked-prefill-size 2048

# Para GPU com 40GB+:
--chunked-prefill-size 8192  # (atual)
```

### 2.3 `max_prefill_tokens` (16384)

| Aspecto | Descri√ß√£o |
|---------|-----------|
| **O que √©** | M√°ximo de tokens em um batch de prefill |
| **Valor atual** | 16384 tokens |
| **Por que importa** | Limita pico de mem√≥ria em requests concorrentes |

**Recomenda√ß√£o:**
- Para profile builder (requests grandes, baixa concorr√™ncia): pode aumentar para 24576
- Para alta concorr√™ncia: manter em 16384 ou reduzir para 8192

### 2.4 `mem_fraction_static` (0.836)

| Aspecto | Descri√ß√£o |
|---------|-----------|
| **O que √©** | Fra√ß√£o da VRAM alocada estaticamente (weights + KV cache pool) |
| **Valor atual** | 83.6% |
| **Por que importa** | Restante √© para ativa√ß√µes din√¢micas |
| **Trade-off** | ‚Üë mais KV cache (throughput); ‚Üì mais margem para picos |

**Recomenda√ß√£o por GPU:**
```bash
# GPU com mem√≥ria apertada (16GB):
--mem-fraction-static 0.75

# GPU confort√°vel (24GB):
--mem-fraction-static 0.83  # (atual ok)

# GPU com folga (40GB+):
--mem-fraction-static 0.88
```

---

## 3. Par√¢metros de Structured Output (XGrammar)

### 3.1 `grammar_backend` (xgrammar)

| Aspecto | Descri√ß√£o |
|---------|-----------|
| **O que √©** | Backend para gera√ß√£o estruturada |
| **Op√ß√µes** | `xgrammar` (default), `outlines`, `llguidance`, `none` |
| **Por que importa** | Garante JSON v√°lido durante gera√ß√£o |

**Compara√ß√£o de Backends:**

| Backend | Velocidade | JSON Schema | Regex | EBNF |
|---------|------------|-------------|-------|------|
| **xgrammar** | ‚ö° Mais r√°pido | ‚úÖ | ‚úÖ | ‚úÖ |
| outlines | üê¢ Mais lento | ‚úÖ | ‚úÖ | ‚ùå |
| llguidance | üê¢ M√©dio | ‚úÖ | ‚úÖ | ‚úÖ |

**Recomenda√ß√£o:** Manter `xgrammar` (j√° √© o default e mais perform√°tico)

### 3.2 `constrained_json_whitespace_pattern` (None)

| Aspecto | Descri√ß√£o |
|---------|-----------|
| **O que √©** | Padr√£o regex para whitespace permitido no JSON |
| **Valor atual** | None (usa padr√£o) |
| **Por que importa** | Controla formata√ß√£o do JSON gerado |

**Op√ß√µes:**
```bash
# JSON compacto (sem espa√ßos extras):
--constrained-json-disable-any-whitespace

# JSON com formata√ß√£o m√≠nima:
--constrained-json-whitespace-pattern "[\n\t ]*"
```

**Recomenda√ß√£o para Profile Builder:**
- Adicionar `--constrained-json-disable-any-whitespace` para JSON mais compacto
- Reduz tokens de sa√≠da em ~10-15%

### 3.3 Como Usar Structured Output no Cliente

```python
# No provider_manager.py, o response_format √© enviado assim:
response_format = {
    "type": "json_schema",
    "json_schema": {
        "name": "company_profile_extraction",
        "schema": CompanyProfile.model_json_schema()  # ~9000 chars
    }
}
```

---

## 4. Par√¢metros de Mem√≥ria e Batching

### 4.1 `cuda_graph_max_bs` (256)

| Aspecto | Descri√ß√£o |
|---------|-----------|
| **O que √©** | Batch size m√°ximo para CUDA graphs |
| **Valor atual** | 256 |
| **Por que importa** | CUDA graphs aceleram decode eliminando overhead de kernel launch |

**Recomenda√ß√£o:**
- Para uso com Profile Builder (baixa concorr√™ncia): `--cuda-graph-max-bs 32`
- Reduz mem√≥ria usada por graphs pr√©-capturados

### 4.2 `max_running_requests` (Auto)

| Aspecto | Descri√ß√£o |
|---------|-----------|
| **O que √©** | N√∫mero m√°ximo de requests rodando simultaneamente |
| **Valor atual** | Auto (baseado na mem√≥ria) |
| **Por que importa** | Controla concorr√™ncia e lat√™ncia |

**Recomenda√ß√£o para Profile Builder:**
```bash
# Limitar para reduzir lat√™ncia e garantir qualidade:
--max-running-requests 8
```

### 4.3 `schedule_policy` (fcfs)

| Op√ß√£o | Descri√ß√£o |
|-------|-----------|
| **fcfs** | First Come First Serve (padr√£o) |
| lpm | Longest Prefix Match (melhor cache hit) |
| random | Aleat√≥rio |

**Recomenda√ß√£o:** Manter `fcfs` para Profile Builder

---

## 5. Configura√ß√£o do Profile Builder

### 5.1 Configura√ß√£o Atual (`profile_llm.json`)

```json
{
  "max_chunk_tokens": 12000,
  "system_prompt_overhead": 3000,
  "group_target_tokens": 8000,
  "use_structured_output": true,
  "structured_output_backend": "xgrammar",
  "recommended_temperature": 0.0
}
```

### 5.2 An√°lise de Alinhamento

| Par√¢metro | Profile Builder | SGLang Server | Status |
|-----------|-----------------|---------------|--------|
| Context | 12k + 3k = 15k | 32k | ‚úÖ OK (47% usado) |
| Temperature | 0.0 | - | ‚úÖ Determin√≠stico |
| Structured Output | json_schema | xgrammar | ‚úÖ Alinhado |
| Max Output | 4096 (llm_limits) | - | ‚úÖ OK |

### 5.3 Configura√ß√£o Recomendada do Servidor para Profile Builder

```bash
python -m sglang.launch_server \
    --model-path Qwen/Qwen2.5-3B-Instruct \
    --port 8000 \
    --host 0.0.0.0 \
    --context-length 32768 \
    --chunked-prefill-size 8192 \
    --max-prefill-tokens 16384 \
    --grammar-backend xgrammar \
    --constrained-json-disable-any-whitespace \
    --max-running-requests 16 \
    --mem-fraction-static 0.83 \
    --cuda-graph-max-bs 64 \
    --log-level info
```

---

## 6. Recomenda√ß√µes de Otimiza√ß√£o

### 6.1 Otimiza√ß√£o de Lat√™ncia (Priorizar TTFT)

```bash
# Adicionar ao comando de launch:
--schedule-policy fcfs \
--disable-radix-cache \           # Se n√£o houver reuso de prefix
--chunked-prefill-size 4096       # Chunks menores = TTFT mais r√°pido
```

**Impacto:** ‚Üì TTFT em ~20-30%, ‚Üë throughput em ~5%

### 6.2 Otimiza√ß√£o de Throughput (Priorizar Requests/s)

```bash
# Adicionar ao comando de launch:
--enable-mixed-chunk \
--chunked-prefill-size 16384 \    # Chunks maiores
--max-running-requests 32 \       # Mais concorr√™ncia
--mem-fraction-static 0.88        # Mais KV cache
```

**Impacto:** ‚Üë throughput em ~40%, ‚Üë lat√™ncia em ~15%

### 6.3 Otimiza√ß√£o de Qualidade (Priorizar JSON V√°lido)

```bash
# Adicionar ao comando de launch:
--grammar-backend xgrammar \
--constrained-json-disable-any-whitespace
```

**No cliente (profile_llm.json):**
```json
{
  "recommended_temperature": 0.0,
  "use_structured_output": true,
  "structured_output_backend": "xgrammar"
}
```

### 6.4 Otimiza√ß√£o de Mem√≥ria (GPU com 16GB)

```bash
python -m sglang.launch_server \
    --model-path Qwen/Qwen2.5-3B-Instruct \
    --mem-fraction-static 0.75 \
    --chunked-prefill-size 2048 \
    --max-prefill-tokens 8192 \
    --cuda-graph-max-bs 16 \
    --max-running-requests 4
```

---

## 7. Troubleshooting

### 7.1 Erro: "404 page not found"

**Causa:** Pod do RunPod n√£o est√° rodando ou endpoint mudou.

**Solu√ß√£o:**
1. Verificar se o pod est√° ativo no dashboard RunPod
2. Verificar URL do proxy (muda ap√≥s restart)
3. Atualizar `VLLM_ENDPOINT` no `.env`

```bash
# Testar endpoint:
curl https://SEU-POD-ID.proxy.runpod.net/v1/models
```

### 7.2 Erro: "Out of Memory"

**Causa:** Mem√≥ria insuficiente para o batch atual.

**Solu√ß√£o:**
```bash
# Reduzir aloca√ß√£o est√°tica:
--mem-fraction-static 0.75

# Reduzir tamanho de chunk:
--chunked-prefill-size 2048

# Limitar concorr√™ncia:
--max-running-requests 4
```

### 7.3 JSON Inv√°lido Apesar de Structured Output

**Causa:** Schema muito complexo ou modelo n√£o seguiu corretamente.

**Solu√ß√µes:**
1. Verificar se `grammar_backend` √© `xgrammar`
2. Usar `temperature: 0.0` para output determin√≠stico
3. Simplificar schema se necess√°rio
4. Verificar logs do servidor para erros de grammar

```bash
# No servidor:
--log-level debug
```

### 7.4 Lat√™ncia Alta em Prompts Longos

**Causa:** Prefill est√° sendo fragmentado demais.

**Solu√ß√£o:**
```bash
# Aumentar tamanho de chunk:
--chunked-prefill-size 16384 \
--max-prefill-tokens 32768
```

---

## üìä Matriz de Configura√ß√£o por Cen√°rio

| Cen√°rio | mem_fraction | chunk_prefill | max_requests | cuda_graph_bs |
|---------|--------------|---------------|--------------|---------------|
| **Dev (16GB GPU)** | 0.75 | 2048 | 4 | 16 |
| **Prod Balanceado** | 0.83 | 8192 | 16 | 64 |
| **Alta Concorr√™ncia** | 0.88 | 4096 | 32 | 128 |
| **Baixa Lat√™ncia** | 0.80 | 4096 | 8 | 32 |
| **Prompts Longos** | 0.85 | 16384 | 8 | 64 |

---

## üìù Checklist de Deploy

- [ ] Pod SGLang rodando no RunPod
- [ ] Endpoint testado com `curl /v1/models`
- [ ] `VLLM_ENDPOINT` configurado no `.env`
- [ ] `VLLM_MODEL` = `Qwen/Qwen2.5-3B-Instruct`
- [ ] `profile_llm.json` com `use_structured_output: true`
- [ ] `llm_limits.json` com `supports_structured_output: true`
- [ ] Temperature = 0.0 para JSON determin√≠stico

---

## üîó Refer√™ncias

- [SGLang Documentation](https://docs.sglang.ai/)
- [Qwen Deployment Guide](https://qwen.readthedocs.io/en/latest/deployment/sglang.html)
- [XGrammar Paper](https://arxiv.org/abs/2411.15100)
- [SGLang ServerArgs Source](https://github.com/sgl-project/sglang/blob/main/python/sglang/srt/server_args.py)

