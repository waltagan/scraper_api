import asyncio
import subprocess
import random
import logging
import time
from typing import List, Tuple, Set, Optional
from urllib.parse import urljoin, urlparse, quote, unquote
from bs4 import BeautifulSoup
from curl_cffi.requests import AsyncSession
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, BrowserConfig
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from tenacity import retry, stop_after_attempt, wait_fixed, wait_exponential, retry_if_exception_type, before_sleep_log
from app.core.proxy import proxy_manager

# Playwright error imports para tratamento espec√≠fico
try:
    from playwright._impl._errors import TargetClosedError, TimeoutError as PlaywrightTimeout
except ImportError:
    # Fallback se os tipos n√£o estiverem dispon√≠veis
    TargetClosedError = Exception
    PlaywrightTimeout = Exception

# Configurar logger
logger = logging.getLogger(__name__)

# Global semaphore to limit concurrent Playwright instances
# Prevents resource exhaustion and browser crashes
playwright_semaphore = asyncio.Semaphore(2)  # Max 2 navegadores simult√¢neos

@retry(
    retry=retry_if_exception_type((TargetClosedError, PlaywrightTimeout)),
    wait=wait_exponential(multiplier=1, min=2, max=10),
    stop=stop_after_attempt(2),
    before_sleep=before_sleep_log(logger, logging.WARNING)
)
async def _playwright_scrape_with_retry(url: str, proxy: Optional[str]) -> Tuple[str, Set[str], Set[str]]:
    """
    Wrapper com retry para chamadas do Playwright.
    Tenta 2x com backoff exponencial antes de desistir.
    Registra tempo total da navega√ß√£o Playwright.
    """
    start_ts = time.perf_counter()
    # Ajustar filtro para ser menos agressivo e preservar mais conte√∫do
    # threshold menor = menos agressivo (mant√©m mais conte√∫do)
    # min_word_threshold menor = aceita textos menores
    md_generator = DefaultMarkdownGenerator(content_filter=PruningContentFilter(threshold=0.35, min_word_threshold=5))
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS, 
        exclude_external_images=True, 
        markdown_generator=md_generator, 
        page_timeout=60000  # ‚úÖ CORRE√á√ÉO 1: Aumentado de 30s para 60s
    )
    browser_config = BrowserConfig(
        browser_type="chromium", 
        headless=True, 
        proxy_config=proxy, 
        user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36"
    )
    
    crawler = None
    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=run_config, magic=True)
            
            if not result.success or not result.markdown or len(result.markdown) < 200:
                raise Exception("Playwright failed or content too short")
            
            pdfs, links = _extract_links(result.markdown, url)
            if not links: 
                pdfs, links = _extract_links_html(result.html, url)
            
            duration = time.perf_counter() - start_ts
            logger.info(
                f"[PERF] scraper step=playwright_main url={url} "
                f"duration={duration:.3f}s markdown_chars={len(result.markdown)} "
                f"pdfs={len(pdfs)} links={len(links)}"
            )
            return result.markdown, pdfs, links
            
    finally:
        # ‚úÖ CORRE√á√ÉO 4: Garantir cleanup adequado
        if crawler is not None:
            try:
                await crawler.close()
            except:
                pass  # J√° foi fechado ou erro ao fechar

async def scrape_url(url: str, max_subpages: int = 100) -> Tuple[str, List[str], List[str]]:
    """
    High-performance scraper with IP Rotation and Parallelism.
    Strategy:
    1. Main Page: Playwright (JS support) with fresh proxy.
    2. Subpages: Parallel Curl Impersonation tasks (max concurrency 10) with unique IPs.
    Registra m√©tricas de tempo por etapa (main, sele√ß√£o de links, subp√°ginas).
    """
    overall_start = time.perf_counter()
    aggregated_markdown = []
    all_pdf_links = set()
    visited_urls = []

    # --- 1. SCRAPE MAIN PAGE (Playwright) ---
    # ‚úÖ CORRE√á√ÉO 3: Usar semaphore para limitar concorr√™ncia
    main_start = time.perf_counter()
    async with playwright_semaphore:
        print(f"[Scraper] Processing Main: {url}")
        main_proxy = await proxy_manager.get_next_proxy()
        
        try:
            # ‚úÖ CORRE√á√ÉO 5: Usar fun√ß√£o com retry autom√°tico
            markdown, pdfs, links = await _playwright_scrape_with_retry(url, main_proxy)
            
            visited_urls.append(url)
            aggregated_markdown.append(f"--- PAGE START: {url} ---\n{markdown}\n--- PAGE END ---\n")
            all_pdf_links.update(pdfs)
            
        except TargetClosedError as e:
            # ‚úÖ CORRE√á√ÉO 2: Tratamento espec√≠fico de TargetClosedError
            logger.warning(f"[Main] Playwright browser closed unexpectedly: {e}. Using Curl fallback.")
            text, pdfs, links = await _system_curl_scrape(url, await proxy_manager.get_next_proxy())
            if text:
                visited_urls.append(url)
                aggregated_markdown.append(f"--- PAGE START: {url} ---\n{text}\n--- PAGE END ---\n")
                all_pdf_links.update(pdfs)
            else:
                return "", [], []
                
        except (PlaywrightTimeout, asyncio.TimeoutError) as e:
            # ‚úÖ CORRE√á√ÉO 2: Tratamento espec√≠fico de timeout
            logger.warning(f"[Main] Playwright timeout: {e}. Using Curl fallback.")
            text, pdfs, links = await _system_curl_scrape(url, await proxy_manager.get_next_proxy())
            if text:
                visited_urls.append(url)
                aggregated_markdown.append(f"--- PAGE START: {url} ---\n{text}\n--- PAGE END ---\n")
                all_pdf_links.update(pdfs)
            else:
                return "", [], []
                
        except Exception as e:
            # ‚úÖ CORRE√á√ÉO 2: Catch-all com fallback
            logger.error(f"[Main] Playwright failed: {e}. Trying System Curl Fallback.")
            text, pdfs, links = await _system_curl_scrape(url, await proxy_manager.get_next_proxy())
            if text:
                visited_urls.append(url)
                aggregated_markdown.append(f"--- PAGE START: {url} ---\n{text}\n--- PAGE END ---\n")
                all_pdf_links.update(pdfs)
            else:
                return "", [], []

    main_duration = time.perf_counter() - main_start
    logger.info(
        f"[PERF] scraper step=main_page url={url} "
        f"duration={main_duration:.3f}s pages=1 pdfs={len(all_pdf_links)} links={len(links)}"
    )

    # --- 2. SCRAPE SUBPAGES (Parallel + Rotation) ---
    # Usar LLM para sele√ß√£o inteligente de links (muito melhor que regras hardcoded!)
    logger.info(f"[Scraper] Encontrados {len(links)} links. Usando LLM para selecionar os mais relevantes...")
    target_subpages = await _select_links_with_llm(links, url, max_links=max_subpages)
    
    if target_subpages:
        subpages_start = time.perf_counter()
        print(f"[Scraper] Processing {len(target_subpages)} subpages with Parallel IP Rotation")
        
        # Semaphore to limit concurrency (avoid overwhelming proxy provider or local resources)
        sem = asyncio.Semaphore(10) 

        async def scrape_subpage(sub_url):
            async with sem:
                try:
                    # Normalizar URL para lidar com caracteres especiais
                    normalized_url = _normalize_url(sub_url)
                    if normalized_url != sub_url:
                        logger.debug(f"[Sub] URL normalizada: {sub_url} -> {normalized_url}")
                    
                    # Get fresh proxy for THIS specific request
                    sub_proxy = await proxy_manager.get_next_proxy()
                    
                    # Try CFFI
                    logger.debug(f"[Sub] Tentando CFFI para: {normalized_url}")
                    text, pdfs, _ = await _cffi_scrape(normalized_url, sub_proxy)
                    
                    # Fallback System Curl
                    if not text:
                        logger.debug(f"[Sub] CFFI falhou, tentando System Curl para: {normalized_url}")
                        text, pdfs, _ = await _system_curl_scrape(normalized_url, sub_proxy)
                    
                    if text:
                        # Verificar se o conte√∫do √© muito pequeno (pode indicar problema)
                        if len(text) < 500:
                            logger.warning(f"‚ö†Ô∏è [Sub] Conte√∫do muito pequeno para {normalized_url}: {len(text)} chars - pode indicar problema de scraping")
                            # Mostrar preview do conte√∫do para debug
                            preview = '\n'.join(text.split('\n')[:10])
                            logger.debug(f"üìÑ Preview do conte√∫do coletado:\n{preview[:500]}")
                        
                        logger.info(f"[Sub] ‚úÖ Success: {normalized_url} ({len(text)} chars)")
                        print(f"[Sub] Success: {normalized_url}")
                        return (normalized_url, text, pdfs)
                    else:
                        logger.warning(f"[Sub] ‚ùå Failed: {normalized_url} - Ambos CFFI e System Curl retornaram vazio")
                        print(f"[Sub] Failed: {normalized_url}")
                        return None
                except Exception as e:
                    logger.error(f"[Sub] ‚ùå Erro ao processar {sub_url}: {type(e).__name__}: {str(e)}", exc_info=True)
                    print(f"[Sub] Failed: {sub_url} - Erro: {type(e).__name__}")
                    return None

        # Launch parallel tasks
        tasks = [scrape_subpage(sub) for sub in target_subpages]
        results = await asyncio.gather(*tasks)

        success_subpages = 0
        for res in results:
            if res:
                sub_url, text, pdfs = res
                visited_urls.append(sub_url)
                aggregated_markdown.append(f"--- PAGE START: {sub_url} ---\n{text}\n--- PAGE END ---\n")
                all_pdf_links.update(pdfs)
                success_subpages += 1

        subpages_duration = time.perf_counter() - subpages_start
        logger.info(
            f"[PERF] scraper step=subpages url={url} "
            f"duration={subpages_duration:.3f}s subpages_requested={len(target_subpages)} "
            f"subpages_ok={success_subpages}"
        )

    total_duration = time.perf_counter() - overall_start
    logger.info(
        f"[PERF] scraper step=total url={url} "
        f"duration={total_duration:.3f}s pages={len(visited_urls)} pdfs={len(all_pdf_links)}"
    )
    return "\n".join(aggregated_markdown), list(all_pdf_links), visited_urls

# --- HELPERS ---

def _normalize_url(url: str) -> str:
    """
    Normaliza URL para lidar com caracteres especiais e encoding.
    Preserva a estrutura da URL mas garante que caracteres especiais sejam codificados corretamente.
    Remove fragmentos malformados que podem aparecer quando o t√≠tulo do link markdown √© inclu√≠do na URL.
    """
    try:
        # Primeiro, limpar a URL removendo espa√ßos e caracteres problem√°ticos no in√≠cio/fim
        url = url.strip()
        
        # Se a URL cont√©m espa√ßos ou aspas, pode ser um problema de parsing
        # Remover aspas e espa√ßos extras
        if url.startswith('"') and url.endswith('"'):
            url = url[1:-1]
        if url.startswith("'") and url.endswith("'"):
            url = url[1:-1]
        url = url.strip()
        
        parsed = urlparse(url)
        
        # Limpar o path removendo partes problem√°ticas como %20%22 (espa√ßo + aspas codificadas)
        # Esses fragmentos geralmente aparecem quando o t√≠tulo do link markdown √© inclu√≠do na URL
        path = parsed.path
        if '%20%22' in path or '%22' in path:
            # Encontrar onde come√ßa o fragmento problem√°tico e remover tudo ap√≥s ele
            problematic_markers = ['%20%22', '%22']
            for marker in problematic_markers:
                if marker in path:
                    # Remover tudo ap√≥s o marcador problem√°tico
                    path = path[:path.index(marker)]
                    break
        
        # Se o path ainda n√£o est√° codificado corretamente, codificar
        if '%' not in path:
            # Separar path em partes e codificar cada parte
            path_parts = path.split('/')
            encoded_parts = [quote(part, safe='') if part else part for part in path_parts]
            path = '/'.join(encoded_parts)
        
        # Limpar query string tamb√©m
        query = parsed.query
        if query:
            # Remover fragmentos problem√°ticos da query
            if '%20%22' in query or '%22' in query:
                query = query.split('%20%22')[0].split('%22')[0]
            if query and '%' not in query:
                # Codificar query string se necess√°rio
                query_parts = query.split('&')
                encoded_query_parts = []
                for part in query_parts:
                    if '=' in part:
                        key, value = part.split('=', 1)
                        encoded_query_parts.append(f"{quote(key, safe='')}={quote(value, safe='')}")
                    else:
                        encoded_query_parts.append(quote(part, safe=''))
                query = '&'.join(encoded_query_parts)
        
        # Reconstruir URL (sem fragment, que pode conter dados problem√°ticos)
        normalized = f"{parsed.scheme}://{parsed.netloc}{path}"
        if query:
            normalized += f"?{query}"
        # N√£o incluir fragment para evitar problemas com t√≠tulos de links markdown
        
        return normalized
    except Exception as e:
        logger.warning(f"Erro ao normalizar URL {url}: {e}, usando URL original")
        return url

@retry(stop=stop_after_attempt(2), wait=wait_fixed(1))
async def _cffi_scrape(url: str, proxy: Optional[str]) -> Tuple[str, Set[str], Set[str]]:
    start_ts = time.perf_counter()
    try:
        async with AsyncSession(impersonate="chrome120", proxy=proxy, timeout=25) as s:
            resp = await s.get(url)
            if resp.status_code != 200:
                raise Exception(f"Status {resp.status_code}")
            text, pdfs, links = _parse_html(resp.text, url)
            duration = time.perf_counter() - start_ts
            logger.debug(
                f"[PERF] scraper step=cffi_subpage url={url} "
                f"duration={duration:.3f}s text_chars={len(text)} pdfs={len(pdfs)} links={len(links)}"
            )
            return text, pdfs, links
    except Exception as e: 
        duration = time.perf_counter() - start_ts
        logger.debug(
            f"[PERF] scraper step=cffi_subpage_fail url={url} "
            f"duration={duration:.3f}s error={type(e).__name__}"
        )
        raise

# Wrapper to handle the retry exception and return empty tuple on final failure
async def _cffi_scrape_safe(url: str, proxy: Optional[str]) -> Tuple[str, Set[str], Set[str]]:
    try:
        return await _cffi_scrape(url, proxy)
    except:
        return "", set(), set()

@retry(stop=stop_after_attempt(2), wait=wait_fixed(1))
async def _system_curl_scrape(url: str, proxy: Optional[str]) -> Tuple[str, Set[str], Set[str]]:
    start_ts = time.perf_counter()
    try:
        cmd = ["curl", "-L", "-k", "-s"]
        if proxy: cmd.extend(["-x", proxy])
        cmd.extend(["-H", "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36", url])
        
        res = await asyncio.to_thread(subprocess.run, cmd, capture_output=True, text=True, timeout=35)
        if res.returncode != 0 or not res.stdout:
            raise Exception("Curl Failed")
        text, pdfs, links = _parse_html(res.stdout, url)
        duration = time.perf_counter() - start_ts
        logger.debug(
            f"[PERF] scraper step=system_curl url={url} "
            f"duration={duration:.3f}s text_chars={len(text)} pdfs={len(pdfs)} links={len(links)}"
        )
        return text, pdfs, links
    except Exception as e: 
        duration = time.perf_counter() - start_ts
        logger.debug(
            f"[PERF] scraper step=system_curl_fail url={url} "
            f"duration={duration:.3f}s error={type(e).__name__}"
        )
        raise

# Wrapper for safe execution
async def _system_curl_scrape_safe(url: str, proxy: Optional[str]) -> Tuple[str, Set[str], Set[str]]:
    try:
        return await _system_curl_scrape(url, proxy)
    except:
        return "", set(), set()

# Note: Update calls in scrape_subpage to use _safe versions if we want to suppress errors after retries
# However, the original code already has try/except blocks inside _cffi_scrape. 
# To use tenacity effectively, we need to remove the internal try/except catch-all and let tenacity catch it.
# BUT, _cffi_scrape is called by scrape_subpage which expects a return value.
# Refactoring _cffi_scrape to raise exception for tenacity, then catching it in the safe wrapper is best.
# Let's revert to the pattern: Decorator on the Logic, Safe Wrapper for the Caller.

# Redefining _cffi_scrape without the broad try/except, but with specific logic
@retry(stop=stop_after_attempt(2), wait=wait_fixed(2))
async def _cffi_scrape_logic(url: str, proxy: Optional[str]) -> Tuple[str, Set[str], Set[str]]:
    async with AsyncSession(impersonate="chrome120", proxy=proxy, timeout=25) as s:
        resp = await s.get(url)
        if resp.status_code != 200: raise Exception(f"Status {resp.status_code}")
        return _parse_html(resp.text, url)

async def _cffi_scrape(url: str, proxy: Optional[str]) -> Tuple[str, Set[str], Set[str]]:
    try:
        return await _cffi_scrape_logic(url, proxy)
    except Exception as e:
        logger.debug(f"[CFFI] Erro ao fazer scrape de {url}: {type(e).__name__}: {str(e)}")
        return "", set(), set()

@retry(stop=stop_after_attempt(2), wait=wait_fixed(2))
async def _system_curl_scrape_logic(url: str, proxy: Optional[str]) -> Tuple[str, Set[str], Set[str]]:
    cmd = ["curl", "-L", "-k", "-s"]
    if proxy: cmd.extend(["-x", proxy])
    cmd.extend(["-H", "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36", url])
    
    res = await asyncio.to_thread(subprocess.run, cmd, capture_output=True, text=True, timeout=35)
    if res.returncode != 0 or not res.stdout: raise Exception("Curl Failed")
    return _parse_html(res.stdout, url)

async def _system_curl_scrape(url: str, proxy: Optional[str]) -> Tuple[str, Set[str], Set[str]]:
    try:
        return await _system_curl_scrape_logic(url, proxy)
    except Exception as e:
        logger.debug(f"[SystemCurl] Erro ao fazer scrape de {url}: {type(e).__name__}: {str(e)}")
        return "", set(), set()


def _parse_html(html: str, url: str) -> Tuple[str, Set[str], Set[str]]:
    try:
        soup = BeautifulSoup(html, 'html.parser')
        for tag in soup(["script", "style", "nav", "footer", "svg"]): tag.extract()
        text = soup.get_text(separator='\n\n')
        clean = '\n'.join(l.strip() for l in text.splitlines() if l.strip())
        return clean, *_extract_links_html(str(soup), url)
    except: return "", set(), set()

def _extract_links_html(html: str, base_url: str) -> Tuple[Set[str], Set[str]]:
    """
    Extrai links HTML e documentos (PDFs, Word, PowerPoint).
    Retorna: (documentos_importantes, links_internos_html)
    """
    documents, internal = set(), set()
    try:
        soup = BeautifulSoup(html, 'html.parser')
        base_domain = urlparse(base_url).netloc
        
        # Documentos importantes que devem ser processados
        DOCUMENT_EXTENSIONS = {'.pdf', '.doc', '.docx', '.ppt', '.pptx'}
        
        # Extens√µes de arquivos que N√ÉO devem ser coletados como p√°ginas HTML
        EXCLUDED_EXTENSIONS = {
            # Imagens
            '.png', '.jpg', '.jpeg', '.gif', '.svg', '.webp', '.ico', '.bmp', '.tiff',
            # Arquivos n√£o-documentos
            '.zip', '.rar', '.tar', '.gz', '.xls', '.xlsx', '.csv', '.txt', '.xml', '.json', '.js', '.css',
            # M√≠dia
            '.mp4', '.mp3', '.avi', '.mov', '.wmv', '.flv', '.webm',
            # Outros
            '.woff', '.woff2', '.ttf', '.eot', '.otf'
        }
        
        for a in soup.find_all('a', href=True):
            full = urljoin(base_url, a['href'])
            parsed = urlparse(full)
            path_lower = parsed.path.lower()
            
            # Verificar se √© documento importante (PDF, Word, PowerPoint)
            if any(path_lower.endswith(ext) for ext in DOCUMENT_EXTENSIONS):
                documents.add(full)
            # Verificar se √© arquivo exclu√≠do (imagem, arquivo, etc)
            elif any(path_lower.endswith(ext) for ext in EXCLUDED_EXTENSIONS):
                continue  # Ignorar imagens e outros arquivos
            # Verificar se √© link interno v√°lido
            elif parsed.netloc == base_domain:
                # Filtrar tamb√©m query strings com extens√µes de imagem
                if not any(ext in parsed.query.lower() for ext in ['.png', '.jpg', '.jpeg', '.gif', '.svg']):
                    internal.add(full)
    except: pass
    return documents, internal

def _extract_links(markdown: str, base_url: str) -> Tuple[Set[str], Set[str]]:
    """Extrair links de markdown formatado"""
    import re
    documents = set()
    internal = set()
    base_domain = urlparse(base_url).netloc
    
    # Documentos importantes que devem ser processados
    DOCUMENT_EXTENSIONS = {'.pdf', '.doc', '.docx', '.ppt', '.pptx'}
    
    # Extens√µes de arquivos que N√ÉO devem ser coletados como p√°ginas HTML
    EXCLUDED_EXTENSIONS = {
        # Imagens
        '.png', '.jpg', '.jpeg', '.gif', '.svg', '.webp', '.ico', '.bmp', '.tiff',
        # Arquivos n√£o-documentos
        '.zip', '.rar', '.tar', '.gz', '.xls', '.xlsx', '.csv', '.txt', '.xml', '.json', '.js', '.css',
        # M√≠dia
        '.mp4', '.mp3', '.avi', '.mov', '.wmv', '.flv', '.webm',
        # Outros
        '.woff', '.woff2', '.ttf', '.eot', '.otf'
    }
    
    # Regex para links markdown: [texto](url) ou [texto](url "t√≠tulo")
    # O t√≠tulo opcional ap√≥s a URL deve ser ignorado
    pattern = r'\[([^\]]+)\]\(([^\)"\s]+)(?:\s+"[^"]*")?\)'
    matches = re.findall(pattern, markdown)
    
    for text, url in matches:
        # Limpar a URL removendo espa√ßos e aspas extras
        url = url.strip().strip('"').strip("'")
        full_url = urljoin(base_url, url)
        parsed = urlparse(full_url)
        path_lower = parsed.path.lower()
        
        # Verificar se √© documento importante (PDF, Word, PowerPoint)
        if any(path_lower.endswith(ext) for ext in DOCUMENT_EXTENSIONS):
            documents.add(full_url)
        # Verificar se √© arquivo exclu√≠do (imagem, arquivo, etc)
        elif any(path_lower.endswith(ext) for ext in EXCLUDED_EXTENSIONS):
            continue  # Ignorar imagens e outros arquivos
        elif parsed.netloc == base_domain:
            # Filtrar tamb√©m query strings com extens√µes de imagem
            if not any(ext in parsed.query.lower() for ext in ['.png', '.jpg', '.jpeg', '.gif', '.svg']):
                internal.add(full_url)
    
    return documents, internal

def _prioritize_links(links: Set[str], base_url: str) -> List[str]:
    # Expanded keyword list for richer profiles
    high = [
        # Core
        "quem-somos", "sobre", "institucional",
        # Offerings (MELHORADO - adicionado cat√°logo)
        "portfolio", "produto", "servico", "solucoes", "atuacao", "tecnologia",
        "catalogo", "catalogo-digital", "catalogo-online", "produtos", "servicos",
        # Trust & Proof (NEW)
        "clientes", "cases", "projetos", "obras", "certificacoes", "premios", "parceiros",
        # Team & Contact (NEW)
        "equipe", "time", "lideranca", "contato", "fale-conosco", "unidades"
    ]
    
    low = ["login", "signin", "cart", "policy", "blog", "news", "politica-privacidade", "termos"]
    
    scored = []
    for l in links:
        if l.rstrip('/') == base_url.rstrip('/'): continue
        s = 0
        lower = l.lower()
        
        if any(k in lower for k in low): s -= 100
        if any(k in lower for k in high): s += 50
        
        # Penalize deep nesting (often less relevant blog posts or detailed product specs)
        s -= len(urlparse(l).path.split('/'))
        
        scored.append((s, l))
        
    return [l for s, l in sorted(scored, key=lambda x: x[0], reverse=True) if s > -80]

def _filter_non_html_links(links: Set[str]) -> Set[str]:
    """
    Filtra links que s√£o imagens, arquivos ou outros recursos n√£o-HTML.
    IMPORTANTE: N√ÉO filtra documentos importantes (PDF, Word, PowerPoint) - eles s√£o processados separadamente.
    """
    # Documentos importantes que N√ÉO devem ser filtrados (s√£o processados separadamente)
    DOCUMENT_EXTENSIONS = {'.pdf', '.doc', '.docx', '.ppt', '.pptx'}
    
    EXCLUDED_EXTENSIONS = {
        # Imagens
        '.png', '.jpg', '.jpeg', '.gif', '.svg', '.webp', '.ico', '.bmp', '.tiff',
        # Arquivos n√£o-documentos
        '.zip', '.rar', '.tar', '.gz', '.xls', '.xlsx', '.csv', '.txt', '.xml', '.json', '.js', '.css',
        # M√≠dia
        '.mp4', '.mp3', '.avi', '.mov', '.wmv', '.flv', '.webm',
        # Outros
        '.woff', '.woff2', '.ttf', '.eot', '.otf'
    }
    
    filtered = set()
    for link in links:
        parsed = urlparse(link)
        path_lower = parsed.path.lower()
        
        # IMPORTANTE: Documentos (PDF, Word, PowerPoint) N√ÉO devem ser filtrados aqui
        # Eles s√£o processados separadamente e n√£o devem aparecer como links HTML
        if any(path_lower.endswith(ext) for ext in DOCUMENT_EXTENSIONS):
            continue  # Documentos s√£o coletados separadamente, n√£o como links HTML
        
        # Verificar extens√£o no path
        if any(path_lower.endswith(ext) for ext in EXCLUDED_EXTENSIONS):
            continue
        
        # Verificar extens√£o na query string
        if any(ext in parsed.query.lower() for ext in ['.png', '.jpg', '.jpeg', '.gif', '.svg', '.webp']):
            continue
        
        # Verificar se o path cont√©m diret√≥rios de assets/imagens comuns
        if any(dir_name in path_lower for dir_name in ['/wp-content/uploads/', '/assets/', '/images/', '/img/', '/static/', '/media/']):
            # Se est√° em diret√≥rio de assets, verificar se tem extens√£o de imagem
            if any(path_lower.endswith(ext) for ext in ['.png', '.jpg', '.jpeg', '.gif', '.svg', '.webp', '.ico']):
                continue
        
        # Incluir o link (n√£o √© arquivo exclu√≠do)
        filtered.add(link)
    
    return filtered

async def _select_links_with_llm(links: Set[str], base_url: str, max_links: int = 30) -> List[str]:
    """
    Usa LLM para selecionar os links mais relevantes para completar o perfil da empresa.
    Muito mais inteligente que regras hardcoded!
    Registra tempo total gasto na sele√ß√£o com LLM ou fallback.
    """
    start_ts = time.perf_counter()
    if not links or len(links) == 0:
        duration = time.perf_counter() - start_ts
        logger.info(
            f"[PERF] scraper step=select_links_llm base_url={base_url} "
            f"duration={duration:.3f}s reason=no_links"
        )
        return []
    
    # FILTRAR PRIMEIRO: Remover imagens e arquivos n√£o-HTML
    filtered_links = _filter_non_html_links(links)
    logger.info(f"Filtrados {len(links) - len(filtered_links)} links n√£o-HTML (imagens, arquivos, etc). Restam {len(filtered_links)} links v√°lidos.")
    
    if not filtered_links:
        duration = time.perf_counter() - start_ts
        logger.info(
            f"[PERF] scraper step=select_links_llm base_url={base_url} "
            f"duration={duration:.3f}s reason=no_filtered_links"
        )
        return []
    
    # Se tiver poucos links ap√≥s filtro, retornar todos
    if len(filtered_links) <= max_links:
        duration = time.perf_counter() - start_ts
        logger.info(
            f"[PERF] scraper step=select_links_llm base_url={base_url} "
            f"duration={duration:.3f}s selected={len(filtered_links)} total_candidates={len(links)} "
            f"strategy=short_circuit"
        )
        return list(filtered_links)
    
    # Importar aqui para evitar depend√™ncia circular
    from openai import AsyncOpenAI
    from app.core.config import settings
    import json
    
    # Preparar cliente LLM
    client = AsyncOpenAI(api_key=settings.LLM_API_KEY, base_url=settings.LLM_BASE_URL)
    
    # Criar prompt com contexto do schema
    links_list = "\n".join([f"{i+1}. {url}" for i, url in enumerate(sorted(filtered_links))])
    
    prompt = f"""Voc√™ √© um especialista em an√°lise de websites B2B.

CONTEXTO: Estamos coletando dados para criar um perfil completo de empresa com os seguintes campos:

**IDENTITY**: Nome da empresa, CNPJ, tagline, descri√ß√£o, ano de funda√ß√£o, n√∫mero de funcion√°rios
**CLASSIFICATION**: Ind√∫stria, modelo de neg√≥cio (B2B/B2C), p√∫blico-alvo, cobertura geogr√°fica
**TEAM**: Tamanho da equipe, cargos-chave, certifica√ß√µes do time
**OFFERINGS**: Produtos, categorias de produtos, servi√ßos, detalhes de servi√ßos, modelos de engajamento, diferenciais
**REPUTATION**: Certifica√ß√µes, pr√™mios, parcerias, lista de clientes, cases de sucesso
**CONTACT**: E-mails, telefones, LinkedIn, endere√ßo, localiza√ß√µes

TAREFA: Selecione os {max_links} links MAIS RELEVANTES da lista abaixo que provavelmente cont√™m informa√ß√µes para preencher esses campos. Priorize:
1. P√°ginas "Sobre", "Quem Somos", "Institucional"
2. P√°ginas de Produtos/Servi√ßos/Solu√ß√µes/Cat√°logos
3. P√°ginas de Cases, Clientes, Projetos
4. P√°ginas de Contato, Equipe, Localiza√ß√µes
5. P√°ginas de Certifica√ß√µes, Pr√™mios, Parcerias

IMPORTANTE: 
- PRIORIZE p√°ginas com palavras como "catalogo", "catalogo-digital", "portfolio", "produtos", "servicos"
- EVITE: Blogs, not√≠cias, pol√≠ticas de privacidade, login, carrinho, termos de uso
- EVITE: Links que parecem ser imagens ou arquivos (j√° foram filtrados, mas seja cuidadoso)

LISTA DE LINKS DO SITE {base_url}:
{links_list}

Responda APENAS com um JSON array contendo os n√∫meros dos links selecionados (ex: [1, 3, 5, 7, ...]):
"""

    try:
        response = await client.chat.completions.create(
            model=settings.LLM_MODEL,
            messages=[
                {"role": "system", "content": "Voc√™ √© um assistente especializado em an√°lise de websites B2B. Responda sempre em JSON v√°lido."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=500,
            response_format={"type": "json_object"}
        )
        
        content = response.choices[0].message.content.strip()
        
        # Parse do JSON
        try:
            result = json.loads(content)
            # Pode vir como {"links": [1,2,3]} ou {"selected": [1,2,3]} ou direto [1,2,3]
            if isinstance(result, list):
                selected_indices = result
            elif "links" in result:
                selected_indices = result["links"]
            elif "selected" in result:
                selected_indices = result["selected"]
            elif "indices" in result:
                selected_indices = result["indices"]
            else:
                # Pegar o primeiro array que encontrar
                for value in result.values():
                    if isinstance(value, list):
                        selected_indices = value
                        break
                else:
                    selected_indices = []
        except:
            # Se falhar no parse, usar estrat√©gia de fallback (prioriza√ß√£o b√°sica)
            logger.warning("LLM n√£o retornou JSON v√°lido, usando fallback")
            duration = time.perf_counter() - start_ts
            prioritized = _prioritize_links(filtered_links, base_url)[:max_links]
            logger.info(
                f"[PERF] scraper step=select_links_llm base_url={base_url} "
                f"duration={duration:.3f}s selected={len(prioritized)} total_candidates={len(links)} "
                f"strategy=fallback_prioritize"
            )
            return prioritized
        
        # Converter √≠ndices para URLs
        sorted_links = sorted(filtered_links)
        selected_urls = []
        for idx in selected_indices:
            if 1 <= idx <= len(sorted_links):
                selected_urls.append(sorted_links[idx - 1])
        
        duration = time.perf_counter() - start_ts
        logger.info(
            f"[PERF] scraper step=select_links_llm base_url={base_url} "
            f"duration={duration:.3f}s selected={len(selected_urls)} total_candidates={len(links)} "
            f"strategy=llm"
        )
        logger.info(f"LLM selecionou {len(selected_urls)} de {len(filtered_links)} links v√°lidos (de {len(links)} totais)")
        return selected_urls[:max_links]
        
    except Exception as e:
        logger.error(f"Erro ao usar LLM para selecionar links: {e}")
        # Fallback para estrat√©gia baseada em regras
        duration = time.perf_counter() - start_ts
        prioritized = _prioritize_links(links, base_url)[:max_links]
        logger.info(
            f"[PERF] scraper step=select_links_llm base_url={base_url} "
            f"duration={duration:.3f}s selected={len(prioritized)} total_candidates={len(links)} "
            f"strategy=error_fallback_prioritize"
        )
        return prioritized
