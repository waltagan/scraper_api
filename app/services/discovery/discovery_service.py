import asyncio
import logging
import json
import urllib.parse
import random
import httpx
from typing import Optional, List, Dict, Any
from bs4 import BeautifulSoup
from openai import AsyncOpenAI

# crawl4ai removido - alto consumo de mem√≥ria (PRD v2.0)
# Fun√ß√£o search_google agora usa Serper como fallback
HAS_CRAWL4AI = False

from app.core.config import settings
from app.core.proxy import proxy_manager
from app.services.llm.provider_manager import provider_manager
from app.services.llm.health_monitor import health_monitor, FailureType
from app.services.llm.queue_manager import create_queue_manager

logger = logging.getLogger(__name__)

# --- CONFIGURA√á√ÉO DE DISCOVERY ---
DISCOVERY_TIMEOUT = 60.0  # Timeout para chamadas LLM de discovery (aumentado para produ√ß√£o)
DISCOVERY_MAX_RETRIES = 3  # N√∫mero m√°ximo de tentativas
DISCOVERY_BACKOFF_BASE = 2  # Base para backoff exponencial (segundos)
DISCOVERY_BACKOFF_MAX = 16  # M√°ximo de backoff (segundos)

# --- CONFIGURA√á√ÉO DO SERPER API ---
# Rate Limit oficial: 300 queries/segundo
# Por√©m, limitamos concorr√™ncia local para evitar exaust√£o de conex√µes
SERPER_CONCURRENT_LIMIT = 200  # conex√µes simult√¢neas m√°ximas (limita√ß√£o local, n√£o da API)
SERPER_REQUEST_TIMEOUT = 15.0  # timeout por request (segundos)
SERPER_CONNECT_TIMEOUT = 5.0   # timeout para estabelecer conex√£o (segundos)
SERPER_MAX_RETRIES = 3         # tentativas m√°ximas por request
SERPER_RETRY_BASE_DELAY = 1.0  # delay base para backoff exponencial (segundos)
SERPER_RETRY_MAX_DELAY = 10.0  # delay m√°ximo para backoff (segundos)

# Sem√°foro global para controle de concorr√™ncia do Serper
_serper_semaphore: Optional[asyncio.Semaphore] = None
_serper_lock = asyncio.Lock()

# Cliente HTTP global com connection pooling
_serper_client: Optional[httpx.AsyncClient] = None
_serper_client_lock = asyncio.Lock()


async def get_serper_semaphore() -> asyncio.Semaphore:
    """Retorna o sem√°foro global do Serper (lazy initialization)."""
    global _serper_semaphore
    async with _serper_lock:
        if _serper_semaphore is None:
            _serper_semaphore = asyncio.Semaphore(SERPER_CONCURRENT_LIMIT)
    return _serper_semaphore


async def get_serper_client() -> httpx.AsyncClient:
    """
    Retorna cliente HTTP global com connection pooling.
    
    Connection pooling evita criar/destruir conex√µes a cada request,
    o que era a causa dos erros de conex√£o em batch de 500 empresas.
    """
    global _serper_client
    async with _serper_client_lock:
        if _serper_client is None or _serper_client.is_closed:
            _serper_client = httpx.AsyncClient(
                timeout=httpx.Timeout(
                    connect=SERPER_CONNECT_TIMEOUT,
                    read=SERPER_REQUEST_TIMEOUT,
                    write=SERPER_REQUEST_TIMEOUT,
                    pool=SERPER_REQUEST_TIMEOUT
                ),
                limits=httpx.Limits(
                    max_keepalive_connections=50,
                    max_connections=SERPER_CONCURRENT_LIMIT,
                    keepalive_expiry=30.0
                ),
                http2=True  # HTTP/2 para melhor performance
            )
            logger.info(f"üåê Serper: Cliente HTTP criado (pool={SERPER_CONCURRENT_LIMIT}, http2=True)")
    return _serper_client


async def close_serper_client():
    """Fecha o cliente HTTP global (chamar no shutdown da aplica√ß√£o)."""
    global _serper_client
    async with _serper_client_lock:
        if _serper_client and not _serper_client.is_closed:
            await _serper_client.aclose()
            _serper_client = None
            logger.info("üåê Serper: Cliente HTTP fechado")


# --- BLACKLIST DE DOM√çNIOS (Pr√©-filtro antes da LLM) ---
# Dom√≠nios que NUNCA devem ser enviados para an√°lise da LLM
BLACKLIST_DOMAINS = {
    # Diret√≥rios Empresariais e Agregadores de CNPJ (alta frequ√™ncia nos resultados)
    "econodata.com.br",
    "cnpj.biz",
    "cnpja.com",
    "cnpj.info",
    "cnpjs.rocks",
    "casadosdados.com.br",
    "empresascnpj.com",
    "consultacnpj.com",
    "informecadastral.com.br",
    "cadastroempresa.com.br",
    "transparencia.cc",
    "listamais.com.br",
    "solutudo.com.br",
    "telelistas.net",
    "apontador.com.br",
    "guiamais.com.br",
    "construtora.net.br",
    "b2bleads.com.br",
    "empresas.serasaexperian.com.br",
    "jusbrasil.com.br",
    "jusdados.com",
    
    # Redes Sociais
    "facebook.com",
    "instagram.com",
    "linkedin.com",
    "youtube.com",
    "twitter.com",
    "x.com",
    "tiktok.com",
    "pinterest.com",
    "threads.net",
    
    # Marketplaces
    "mercadolivre.com.br",
    "shopee.com.br",
    "olx.com.br",
    "amazon.com.br",
    "magazineluiza.com.br",
    "americanas.com.br",
    
    # Google Services (tradutor, cache, etc)
    "translate.google.com",
    "webcache.googleusercontent.com",
}


def is_blacklisted_domain(url: str) -> bool:
    """
    Verifica se a URL pertence a um dom√≠nio na blacklist.
    Retorna True se deve ser filtrado (n√£o enviar para LLM).
    """
    if not url:
        return False
    
    try:
        # Normaliza URL
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        parsed = urllib.parse.urlparse(url)
        domain = parsed.netloc.lower()
        
        # Remove prefixos comuns
        for prefix in ('www.', 'm.', 'mobile.'):
            if domain.startswith(prefix):
                domain = domain[len(prefix):]
        
        # Verifica match exato ou subdom√≠nio
        for blacklisted in BLACKLIST_DOMAINS:
            if domain == blacklisted or domain.endswith('.' + blacklisted):
                return True
        
        return False
        
    except Exception:
        return False



DISCOVERY_PROMPT = """Voc√™ √© um especialista em encontrar sites oficiais de empresas brasileiras.

# TAREFA
Analise os resultados de busca e identifique o site OFICIAL da empresa.

# REGRA DE OURO (OBRIGAT√ìRIA - SIGA SEMPRE)
Se o DOM√çNIO cont√©m o NOME da empresa (mesmo que junto ou abreviado), ACEITE IMEDIATAMENTE.
Remova espa√ßos e compare: "AR ENGENHARIA" ‚Üí "arengenharia" ‚Üí dom√≠nio "arengenharia.eng.br" = ‚úÖ MATCH

EXEMPLOS DE MATCH (TODOS devem ser ACEITOS):
- "12M" ‚Üí "12m.com.br" ‚úÖ
- "AR ENGENHARIA" ‚Üí "arengenharia.eng.br" ‚úÖ (nome sem espa√ßos = dom√≠nio)
- "CONSTRUTORA CESAR" ‚Üí "construtoracesar.com.br" ‚úÖ (nome completo no dom√≠nio)
- "ASST Servi√ßos" ‚Üí "asst.com.br" ‚úÖ (sigla no dom√≠nio)
- "CIMMAA Metalmecanica" ‚Üí "cimmaa.com.br" ‚úÖ (nome principal no dom√≠nio)
- "Alianza Manuten√ß√£o" ‚Üí "allianzautomacao.com.br" ‚úÖ (varia√ß√£o ortogr√°fica)
- "4M Engenharia" ‚Üí "4mengenharia.com.br" ‚úÖ

# PROCESSO DE DECIS√ÉO

## PASSO 1: Remover diret√≥rios e redes sociais
IGNORE completamente URLs contendo: facebook, instagram, linkedin, youtube, twitter, x.com, tiktok, cnpj.biz, econodata, telelistas, apontador, solutudo, mercadolivre, shopee, olx

## PASSO 2: Para cada URL restante, fa√ßa o match
1. Extraia o dom√≠nio (ex: "arengenharia.eng.br")
2. Remova sufixos (.com.br, .eng.br, etc) ‚Üí "arengenharia"
3. Compare com Nome Fantasia SEM ESPA√áOS ‚Üí "arengenharia"
4. Se s√£o iguais ou muito similares ‚Üí ACEITE IMEDIATAMENTE

## PASSO 3: Se m√∫ltiplos matches, escolha o primeiro (mais bem ranqueado)

# IMPORTANTE
- N√ÉO exija que o snippet confirme o site - snippets do Google s√£o frequentemente ERRADOS
- N√ÉO rejeite um site s√≥ porque o t√≠tulo n√£o √© id√™ntico ao nome da empresa
- Se o dom√≠nio cont√©m o nome, ACEITE - n√£o h√° necessidade de mais evid√™ncias

# RESPOSTA (JSON obrigat√≥rio)
```json
{
  "site": "URL_DO_SITE ou nao_encontrado",
  "site_oficial": "sim ou nao",
  "justificativa": "Breve explica√ß√£o"
}
```
"""
    
    # import httpx  <-- Removido, j√° est√° no topo


async def search_google_serper(query: str, num_results: int = 100) -> List[Dict[str, str]]:
    """
    Realiza uma busca no Google usando a API Serper.dev.
    
    Melhorias v2.1:
    - Connection pooling (cliente HTTP global)
    - Retry com backoff exponencial
    - Logging detalhado de erros (tipo + mensagem)
    - Controle de concorr√™ncia local
    """
    if not settings.SERPER_API_KEY:
        logger.warning("‚ö†Ô∏è SERPER_API_KEY n√£o configurada.")
        return await search_google(query, num_results)

    # Obter sem√°foro de concorr√™ncia
    semaphore = await get_serper_semaphore()
    
    async with semaphore:
        logger.debug(f"üîé Serper query: {query[:50]}...")
        
        url = "https://google.serper.dev/search"
        payload = json.dumps({
            "q": query,
            "num": num_results,
            "gl": "br",
            "hl": "pt-br"
        })
        headers = {
            'X-API-KEY': settings.SERPER_API_KEY,
            'Content-Type': 'application/json'
        }

        # Obter cliente HTTP com connection pooling
        client = await get_serper_client()
        
        last_error = None
        last_error_type = None
        
        for attempt in range(SERPER_MAX_RETRIES):
            try:
                # Calcular delay de backoff (0 na primeira tentativa)
                if attempt > 0:
                    delay = min(
                        SERPER_RETRY_BASE_DELAY * (2 ** (attempt - 1)) + random.uniform(0, 0.5),
                        SERPER_RETRY_MAX_DELAY
                    )
                    logger.debug(f"üîÑ Serper retry {attempt + 1}/{SERPER_MAX_RETRIES} ap√≥s {delay:.1f}s")
                    await asyncio.sleep(delay)
                
                response = await client.post(url, headers=headers, content=payload)
                
                # Rate limit - fazer backoff e retry
                if response.status_code == 429:
                    logger.warning(f"‚ö†Ô∏è Serper rate limit (429), tentativa {attempt + 1}/{SERPER_MAX_RETRIES}")
                    last_error = "Rate limit (429)"
                    last_error_type = "RateLimit"
                    continue
                
                # Erro de servidor - pode ser tempor√°rio
                if response.status_code >= 500:
                    logger.warning(f"‚ö†Ô∏è Serper server error ({response.status_code}), tentativa {attempt + 1}/{SERPER_MAX_RETRIES}")
                    last_error = f"Server error ({response.status_code})"
                    last_error_type = "ServerError"
                    continue
                
                # Erro de cliente (4xx exceto 429) - n√£o faz retry
                if response.status_code >= 400:
                    logger.error(f"‚ùå Serper client error: {response.status_code}")
                    return []
                
                # Sucesso
                data = response.json()
                organic_results = data.get("organic", [])
                
                results = []
                for item in organic_results:
                    results.append({
                        "title": item.get("title"),
                        "link": item.get("link"),
                        "snippet": item.get("snippet", "")
                    })
                
                logger.debug(f"‚úÖ Serper: {len(results)} resultados")
                return results
                
            except httpx.TimeoutException as e:
                last_error_type = "Timeout"
                last_error = f"timeout ap√≥s {SERPER_REQUEST_TIMEOUT}s"
                logger.warning(f"‚ö†Ô∏è Serper {last_error_type}: {last_error}, tentativa {attempt + 1}/{SERPER_MAX_RETRIES}")
                
            except httpx.ConnectError as e:
                last_error_type = "ConnectError"
                last_error = str(e) if str(e) else "falha ao conectar"
                logger.warning(f"‚ö†Ô∏è Serper {last_error_type}: {last_error}, tentativa {attempt + 1}/{SERPER_MAX_RETRIES}")
                
            except httpx.PoolTimeout as e:
                last_error_type = "PoolTimeout"
                last_error = "pool de conex√µes esgotado"
                logger.warning(f"‚ö†Ô∏è Serper {last_error_type}: {last_error}, tentativa {attempt + 1}/{SERPER_MAX_RETRIES}")
                
            except httpx.HTTPStatusError as e:
                last_error_type = "HTTPStatusError"
                last_error = f"status {e.response.status_code}"
                logger.warning(f"‚ö†Ô∏è Serper {last_error_type}: {last_error}, tentativa {attempt + 1}/{SERPER_MAX_RETRIES}")
                
            except Exception as e:
                # Captura qualquer outra exce√ß√£o com tipo expl√≠cito
                last_error_type = type(e).__name__
                last_error = str(e) if str(e) else "erro desconhecido"
                logger.warning(f"‚ö†Ô∏è Serper {last_error_type}: {last_error}, tentativa {attempt + 1}/{SERPER_MAX_RETRIES}")
        
        # Todas as tentativas falharam
        logger.error(f"‚ùå Serper falhou ap√≥s {SERPER_MAX_RETRIES} tentativas: [{last_error_type}] {last_error}")
        return []

async def search_google(query: str, num_results: int = 10) -> List[Dict[str, str]]:
    """
    Realiza uma busca no Google.
    
    NOTA v2.0: crawl4ai foi removido por alto consumo de mem√≥ria.
    Esta fun√ß√£o agora retorna lista vazia - use search_google_serper como principal.
    """
    logger.warning(
        "‚ö†Ô∏è search_google: crawl4ai n√£o dispon√≠vel (PRD v2.0). "
        "Use search_google_serper como m√©todo principal de busca."
    )
    
    # Fallback: tentar busca simples via httpx (sem browser)
    try:
        import httpx
        
        encoded_query = urllib.parse.quote(query)
        url = f"https://www.google.com/search?q={encoded_query}&hl=pt-BR&num={num_results}"
        
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept": "text/html,application/xhtml+xml",
            "Accept-Language": "pt-BR,pt;q=0.9"
        }
        
        async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
            response = await client.get(url, headers=headers)
            
            if response.status_code != 200:
                logger.warning(f"search_google: Status {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            for script in soup(["script", "style"]):
                script.decompose()
            
            results = []
            results_found = False
            possible_selectors = ['div.g', 'div.MjjYud', 'div.tF2Cxc']
            
            for selector in possible_selectors:
                items = soup.select(selector)
                if items:
                    for item in items:
                        h3 = item.find('h3')
                        a = item.find('a', href=True)
                        
                        if h3 and a:
                            link = a['href']
                            title = h3.get_text(strip=True)
                            container_text = item.get_text(separator=' ', strip=True)
                            snippet = container_text.replace(title, '', 1).strip()
                            snippet = snippet.replace(link, '')
                            
                            if link.startswith('http') and 'google.com' not in link:
                                results.append({
                                    "title": title,
                                    "link": link,
                                    "snippet": snippet[:400]
                                })
                                results_found = True
                    
                    if results_found:
                        break

            # Fallback Agressivo
            if not results_found:
                logger.warning("‚ö†Ô∏è Seletores espec√≠ficos falharam. Usando extra√ß√£o por proximidade.")
                # Procura todos h3 (que geralmente s√£o t√≠tulos)
                all_h3 = soup.find_all('h3')
                for h3 in all_h3:
                    # O link costuma ser o pai ou vizinho
                    parent_a = h3.find_parent('a', href=True)
                    if not parent_a:
                        # √Äs vezes o h3 est√° dentro do a, ou o a est√° logo antes
                        continue
                        
                    link = parent_a['href']
                    title = h3.get_text(strip=True)
                    
                    if not link.startswith('http') or 'google.com' in link:
                        continue

                    # Tenta pegar o snippet: texto no elemento pai do link (container do resultado)
                    container = parent_a.find_parent('div')
                    snippet = ""
                    if container:
                        full_text = container.get_text(separator=' ', strip=True)
                        snippet = full_text.replace(title, '', 1).replace(link, '').strip()
                    
                    results.append({
                        "title": title,
                        "link": link,
                        "snippet": snippet[:400]
                    })
                        
    except Exception as e:
        logger.error(f"‚ùå Erro na execu√ß√£o da busca: {e}")
        return []

    logger.info(f"‚úÖ Encontrados {len(results)} resultados na busca.")
    return results

async def find_company_website(
    razao_social: str, 
    nome_fantasia: str, 
    cnpj: str,
    email: Optional[str] = None,
    municipio: Optional[str] = None,
    cnaes: Optional[List[str]] = None,
    ctx_label: str = ""
) -> Optional[str]:
    """
    Orquestra a descoberta do site oficial da empresa.
    
    Queries otimizadas (m√°ximo 2):
    - Q1: Nome Fantasia + cidade (se existir)
    - Q2: Raz√£o Social + cidade (se existir)
    
    Se tiver apenas um dado, faz apenas uma query.
    """
    queries = []
    
    nf = nome_fantasia.strip() if nome_fantasia else ""
    rs = razao_social.strip() if razao_social else ""
    city = municipio.strip() if municipio else ""
    
    # Q1: Nome Fantasia + Municipio
    if nf:
        q1 = f'{nf} {city} site oficial'.strip()
        queries.append(q1)
        logger.debug(f"{ctx_label}üìù Q1: {q1}")
    
    # Q2: Raz√£o Social + Municipio (apenas se diferente do nome fantasia)
    if rs:
        # Limpar sufixos jur√≠dicos
        clean_rs = rs.replace(" LTDA", "").replace(" S.A.", "").replace(" EIRELI", "")
        clean_rs = clean_rs.replace(" ME", "").replace(" EPP", "").replace(" S/A", "").strip()
        
    # S√≥ adiciona Q2 se for diferente de Q1
    q2 = f'{clean_rs} {city} site oficial'.strip()
    if not nf or clean_rs.upper() != nf.upper():
        queries.append(q2)
        logger.debug(f"{ctx_label}üìù Q2: {q2}")

    # Q3: Busca por CNPJ (pode revelar site no rodap√© ou p√°gina de contato)
    if cnpj:
        q3 = f'"{cnpj}" site'.strip()
        queries.append(q3)
        logger.debug(f"{ctx_label}üìù Q3: {q3}")
    
    # Se n√£o gerou queries (input vazio), retorna
    if not queries:
        logger.warning(f"{ctx_label}‚ö†Ô∏è Sem Nome Fantasia ou Raz√£o Social para busca.")
        return None
    
    logger.info(f"{ctx_label}üîç Discovery: {len(queries)} query(s) para {nf or rs}")

    # ESTRAT√âGIA EXTRA: Valida√ß√£o de E-mail (Apenas Log)
    # Se tiver email corporativo, logamos para debug, mas n√£o for√ßamos busca espec√≠fica.
    if email and "@" in email:
        domain_part = email.split("@")[1].lower().strip()
        generic_domains = [
            "gmail.com", "outlook.com", "hotmail.com", "yahoo.com", "yahoo.com.br", 
            "uol.com.br", "bol.com.br", "terra.com.br", "ig.com.br", "icloud.com", "me.com"
        ]
        if domain_part not in generic_domains and "." in domain_part:
            logger.info(f"{ctx_label}üìß Dom√≠nio de email dispon√≠vel para valida√ß√£o cruzada: {domain_part}")

    # Executar buscas (sequencial para evitar rate limit agressivo)
    all_results = []
    seen_links = set()
    filtered_count = 0
    
    # Executar queries em ordem de prioridade
    
    for q in queries:
        res = await search_google_serper(q)
        for r in res:
            link = r.get('link', '')
            if link not in seen_links:
                # Pr√©-filtro: remover dom√≠nios da blacklist antes de enviar para LLM
                if is_blacklisted_domain(link):
                    filtered_count += 1
                    continue
                all_results.append(r)
                seen_links.add(link)
    
    if filtered_count > 0:
        logger.debug(f"{ctx_label}üö´ {filtered_count} resultados filtrados (blacklist)")
    
    if not all_results:
        logger.warning(f"{ctx_label}‚ö†Ô∏è Nenhum resultado encontrado no Google ap√≥s m√∫ltiplas buscas.")
        return None
        
    # Logar resultados consolidados para debug
    logger.info(f"{ctx_label}üîç Resultados consolidados enviados para IA ({len(all_results)} itens)")
    logger.debug(json.dumps(all_results, indent=2, ensure_ascii=False))

    # 3. Analisar com LLM (com load balancing e retry)
    results_text = json.dumps(all_results, indent=2, ensure_ascii=False)
    
    user_content = f"""
    Dados da Empresa:
    - Nome Fantasia: {nome_fantasia or 'N√£o informado'}
    - Raz√£o Social: {razao_social or 'N√£o informado'}
    - CNPJ: {cnpj or 'N√£o informado'}
    - E-mail: {email or 'N√£o informado'}
    - Munic√≠pio: {municipio or 'N√£o informado'}
    - CNAEs (Atividades): {', '.join(cnaes) if cnaes else 'N√£o informado'}
    
    Resultados da Busca (Consolidados):
    {results_text}
    """
    
    # Retry com backoff exponencial e load balancing WEIGHTED
    last_error = None
    providers_tried = []
    
    # Criar queue_manager para usar weighted selection
    queue_manager = create_queue_manager(
        providers=provider_manager.available_providers,
        priorities=provider_manager.provider_weights  # Usar WEIGHTS ao inv√©s de priorities
    )
    
    for attempt in range(DISCOVERY_MAX_RETRIES):
        # NOVO: Usar weighted selection para distribuir carga proporcionalmente
        selected_provider = queue_manager.get_weighted_provider(
            exclude=providers_tried,
            weights=provider_manager.provider_weights
        )
        
        if not selected_provider:
            logger.error(f"{ctx_label}‚ùå Nenhum provider LLM dispon√≠vel")
            continue
        
        providers_tried.append(selected_provider)
        
        # Calcular backoff para retry (0 na primeira tentativa)
        if attempt > 0:
            backoff = min(DISCOVERY_BACKOFF_BASE ** attempt + random.uniform(0, 1), DISCOVERY_BACKOFF_MAX)
            logger.debug(f"{ctx_label}üîÑ Discovery retry {attempt + 1}/{DISCOVERY_MAX_RETRIES} ap√≥s {backoff:.1f}s")
            await asyncio.sleep(backoff)
        
        try:
            start_time = asyncio.get_event_loop().time()
            
            messages = [
                            {"role": "system", "content": DISCOVERY_PROMPT},
                            {"role": "user", "content": user_content}
            ]
            
            content, latency_ms = await provider_manager.call(
                provider=selected_provider,
                messages=messages,
                timeout=DISCOVERY_TIMEOUT,
                response_format={"type": "json_object"},
                ctx_label=ctx_label
            )
            
            duration = asyncio.get_event_loop().time() - start_time
            logger.info(f"{ctx_label}üß† Decis√£o do LLM ({selected_provider}): {content}")
            
            # Registrar sucesso
            health_monitor.record_success(selected_provider, latency_ms)
            
            try:
                data = json.loads(content)
            except json.JSONDecodeError:
                # Tentar limpar markdown se houver
                if "```json" in content:
                    content = content.split("```json")[1].split("```")[0].strip()
                    data = json.loads(content)
                else:
                    raise

            # Tratamento para caso a IA retorne uma lista em vez de um objeto
            if isinstance(data, list):
                if len(data) > 0:
                    data = data[0]
                else:
                    logger.warning("‚ö†Ô∏è IA retornou lista vazia.")
                    return None
            
            if data.get("site_oficial") == "sim" and data.get("site") and data.get("site") != "nao_encontrado":
                return data.get("site")
            else:
                logger.debug(f"{ctx_label}Site n√£o encontrado. Justificativa: {data.get('justificativa')}")
                return None
            
        except asyncio.TimeoutError:
            duration = (asyncio.get_event_loop().time() - start_time) * 1000 if 'start_time' in dir() else DISCOVERY_TIMEOUT * 1000
            health_monitor.record_failure(selected_provider, FailureType.TIMEOUT, duration)
            logger.warning(f"{ctx_label}‚ö†Ô∏è Timeout na an√°lise do LLM ({selected_provider}) para descoberta de site ({DISCOVERY_TIMEOUT}s). "
                          f"Tentativa {attempt + 1}/{DISCOVERY_MAX_RETRIES}")
            last_error = "timeout"
            continue  # Tentar novamente com outro provedor
            
        except Exception as e:
            duration = (asyncio.get_event_loop().time() - start_time) * 1000 if 'start_time' in dir() else 0
            health_monitor.record_failure(selected_provider, FailureType.ERROR, duration)
            logger.warning(f"{ctx_label}‚ö†Ô∏è Erro na an√°lise do LLM ({selected_provider}): {e}. "
                          f"Tentativa {attempt + 1}/{DISCOVERY_MAX_RETRIES}")
            last_error = str(e)
            continue  # Tentar novamente com outro provedor
    
    # Todas as tentativas falharam
    logger.error(f"{ctx_label}‚ùå Erro na an√°lise do LLM para descoberta de site ap√≥s {DISCOVERY_MAX_RETRIES} tentativas. "
                f"√öltimo erro: {last_error}")
    return None
