import asyncio
import logging
import json
import urllib.parse
from typing import Optional, List, Dict, Any
from bs4 import BeautifulSoup
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from openai import AsyncOpenAI

from app.core.config import settings
from app.core.proxy import proxy_manager

logger = logging.getLogger(__name__)

DISCOVERY_PROMPT = """# Prompt: Agente Especialista em Localizar Sites Oficiais de Empresas

## Objetivo
Voc√™ √© um agente especializado em localizar **o site oficial de uma empresa brasileira**, com base em dados cadastrais e resultados de busca. Seu papel √© garantir que o site identificado seja **efetivamente controlado pela empresa**, mesmo que o nome da empresa apare√ßa com pequenas varia√ß√µes.

## Informa√ß√µes de entrada
Voc√™ receber√°:
- Nome Fantasia
- Raz√£o Social
- CNPJ (quando dispon√≠vel)
- E-mail (quando dispon√≠vel) - Verifique se o dom√≠nio do e-mail coincide com o site.
- Munic√≠pio - Verifique se o site menciona a cidade ou regi√£o.
- CNAEs (atividades) - Verifique se o site oferece servi√ßos compat√≠veis com estas atividades.
- Lista de Resultados da Busca (T√≠tulo, URL, Snippet)

## Estrat√©gia de Busca e Verifica√ß√£o

1. **Busque dom√≠nios corporativos pr√≥prios**, incluindo sufixos como:
   `.com.br`, `.com`, `.net.br`, `.eng.br`, `.ind.br`, `.wixsite.com` (com valida√ß√£o refor√ßada).

2. **Aceite varia√ß√µes no nome da empresa no dom√≠nio**, desde que:
   - O nome esteja parcialmente presente no dom√≠nio (ex: `rubiengenharia.com.br` para "Rubi Engenharia").
   - O conte√∫do do site comprove a identidade por outros meios (ver abaixo).

3. **Valide a identidade com base em ao menos duas das evid√™ncias abaixo**:
   - Nome da empresa (ou varia√ß√£o) na se√ß√£o "Sobre", "Contato" ou rodap√© do site;
   - Endere√ßo compat√≠vel com a cidade ou estado fornecidos;
   - E-mail corporativo no dom√≠nio (ex: contato@empresa.com.br);
   - Presen√ßa de CNPJ ou raz√£o social completa no site;
   - Perfis oficiais da empresa (Instagram, LinkedIn, etc.) apontando diretamente para o dom√≠nio;
   - Servi√ßos, clientes ou projetos listados que confirmam o segmento informado.

4. **Sites em plataformas como Wix ou WordPress** s√£o aceit√°veis **se o conte√∫do comprovar a identidade institucional** com base nas evid√™ncias acima.

5. **Similaridade de Dom√≠nio (Alta Prioridade)**:
   - Se a URL cont√©m o "Nome Fantasia" ou "Raz√£o Social" de forma clara (ex: Empresa "Eco Mineral", Site "ecomineral.com.br"), considere isso o fator MAIS FORTE.
   - **IMPORTANTE:** Se o dom√≠nio for um match quase exato com o nome, **IGNORE** diverg√™ncias leves no snippet ou descri√ß√£o de atividade. O snippet do Google muitas vezes √© impreciso. D√™ o benef√≠cio da d√∫vida para dom√≠nios com nome igual.

6. **Atividade Econ√¥mica (CNAEs/Descri√ß√£o) - Fator Secund√°rio**:
   - Use os CNAEs apenas para confirma√ß√£o sutil ("tie-breaker").
   - **N√ÉO DESCARTE** um site apenas porque a descri√ß√£o do snippet n√£o bate perfeitamente com os CNAEs.
   - Exemplo: Se a empresa √© "Minera√ß√£o" (CNAE) e o site fala de "Produtos Abrasivos" ou "Solu√ß√µes Ambientais", considere V√ÅLIDO se o nome bater. As empresas muitas vezes t√™m bra√ßos comerciais diferentes do CNAE principal.

7. **Valida√ß√£o de E-mail (Se fornecido)**:
   - O campo "email" √© APENAS para valida√ß√£o.
   - Se o dom√≠nio do site encontrado bater com o dom√≠nio do e-mail (ex: site 'empresa.com.br', email 'contato@empresa.com.br'), √© uma confirma√ß√£o definitiva (100% certeza).
   - Se o dom√≠nio do site for DIFERENTE do e-mail (ex: email gmail, ou email de consultoria), isso **N√ÉO** invalida o site.

8. Se houver **m√∫ltiplos sites candidatos**, selecione o que possuir mais evid√™ncias cruzadas de v√≠nculo com a empresa.

## Exclus√µes Obrigat√≥rias

Rejeite os seguintes tipos de p√°ginas, mesmo se mencionarem a empresa:
- Diret√≥rios empresariais (ex: CNPJ.biz, Econodata, TeleListas, Apontador, SerasaExperian)
- Sites de marketplaces (ex: OLX, Mercado Livre, Shopee)
- Sites de releases, not√≠cias ou mat√©rias jornal√≠sticas
- Blogs ou p√°ginas pessoais que apenas mencionem a empresa
- P√°ginas com nomes semelhantes, mas sem qualquer evid√™ncia de v√≠nculo com os dados fornecidos

## Exemplos para calibra√ß√£o

**Aceite:**
- `https://www.brzfire.com.br` ‚Üí dom√≠nio combina com nome fantasia, e-mail institucional aparece no rodap√©.
- `http://www.rubiengenharia.com.br` ‚Üí nome parcialmente compat√≠vel, site descreve servi√ßos de engenharia e localiza√ß√£o compat√≠vel.
- `https://ethicusscs.wixsite.com/refrigeracao` ‚Üí site Wix, mas com nome da empresa, segmento e contato institucional v√°lidos.

**N√ÉO ACEITE DE MANEIRA ALGUMA:**
RESULTADOS QUE EXIBEM UM DOMINIO ENQUADRADO DENTRO DOS TIPOS ABAIXO S√ÉO CONSIDERADOS COMO "N√ÉO CONFIAVEIS" E N√ÉO DEVEM SER RETORNADOS, AO INV√àS DISSO RESPONDA COM "nao_encontrado".
- `https://cnpj.biz/empresa-nome` ‚Üí diret√≥rio empresarial sem v√≠nculo institucional.
- `https://shopee.com/empresa-xyz` ‚Üí perfil em marketplace, n√£o institucional.
- `https://econodata.com.br/empresa-xyz` ‚Üí diret√≥rio empresarial sem v√≠nculo institucional.
- `https://facebook.com/empresa-xyz` ‚Üí perfil em redes sociais, n√£o institucional.

## Formato de Resposta (JSON)
O parametro "site_oficial" deve ser "sim" para sites que com certeza pertencem a empresa em si, n√£o representando sites de terceiros ou men√ß√µes ou diret√≥rios ou qualquer coisa nesse sentido. Sites que n√£o est√£o em contro da empresa e s√£o diret√≥rios ou sites de terceitos, devem ter "site_oficial" = "nao".

Se o site for considerado oficial:
```json
{
  "site": "https://www.nomedasuaempresa.com.br",
  "justificativa": "O dom√≠nio cont√©m o nome fantasia e o site descreve os servi√ßos, endere√ßo e e-mail institucional compat√≠veis com os dados fornecidos...",
 "site_oficial" : "sim"
}
```

Se nenhum site for confi√°vel:
```json
{
  "site": "nao_encontrado",
  "justificativa": "Nenhum dos sites encontrados possui evid√™ncia suficiente de pertencimento √† empresa. Todos s√£o diret√≥rios, dom√≠nios gen√©ricos ou men√ß√µes indiretas.",
 "site_oficial" : "nao"
}
```

Apenas diret√≥rios, redes sociais, ou marketplaces foram encontrados:
```json
{
"site": "nao_encontrado",
"justificativa": "Os dom√≠nios encontrados representam diret√≥rios empresariais...",
 "site_oficial" : "nao"
}
```
"""

import httpx

async def search_google_serper(query: str, num_results: int = 20) -> List[Dict[str, str]]:
    """
    Realiza uma busca no Google usando a API Serper.dev (mais confi√°vel).
    """
    if not settings.SERPER_API_KEY:
        logger.warning("‚ö†Ô∏è SERPER_API_KEY n√£o configurada. Usando fallback para scraping (pode falhar).")
        return await search_google(query, num_results) # Fallback para o antigo se n√£o tiver key

    logger.info(f"üîé Buscando no Google via Serper: {query}")
    
    url = "https://google.serper.dev/search"
    payload = json.dumps({
        "q": query,
        "num": num_results,
        "gl": "br",
        "hl": "pt-br"
    })
    headers = {
        'X-API-KEY': settings.SERPER_API_KEY,
        'Content-Type': 'application/json'
    }

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(url, headers=headers, data=payload, timeout=30.0)
            
            if response.status_code != 200:
                logger.error(f"‚ùå Erro na Serper API: {response.status_code} - {response.text}")
                return []
            
            data = response.json()
            organic_results = data.get("organic", [])
            
            results = []
            for item in organic_results:
                results.append({
                    "title": item.get("title"),
                    "link": item.get("link"),
                    "snippet": item.get("snippet", "")
                })
            
            logger.info(f"‚úÖ Serper retornou {len(results)} resultados.")
            return results
            
    except Exception as e:
        logger.error(f"‚ùå Erro na execu√ß√£o da busca Serper: {e}")
        return []

async def search_google(query: str, num_results: int = 10) -> List[Dict[str, str]]:
    """
    Realiza uma busca no Google e extrai os resultados org√¢nicos.
    Usa crawl4ai com proxy para evitar bloqueios.
    """
    encoded_query = urllib.parse.quote(query)
    url = f"https://www.google.com/search?q={encoded_query}&hl=pt-BR&num={num_results}"
    
    logger.info(f"üîé Buscando no Google: {query}")
    
    # Configura√ß√£o do Crawler
    # Usar user-agent realista e proxy rotativo
    proxy = await proxy_manager.get_next_proxy()
    
    browser_config = BrowserConfig(
        browser_type="chromium",
        headless=True,
        proxy_config=proxy,
        user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    )
    
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        # Atraso aleat√≥rio para simular humano? O crawl4ai j√° tem algumas prote√ß√µes.
        # Vamos confiar no proxy e no browser.
        page_timeout=30000
    )
    
    results = []
    
    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(url=url, config=run_config)
            
            if not result.success:
                logger.error(f"‚ùå Falha ao buscar no Google: {result.error_message}")
                return []
            
            # Parse HTML
            soup = BeautifulSoup(result.html, 'html.parser')
            
            # Remover scripts e estilos para limpar o texto
            for script in soup(["script", "style"]):
                script.decompose()

            # Estrat√©gia de Extra√ß√£o em Camadas
            # O Google muda muito o HTML (div.g, div.MjjYud, etc).
            # Vamos tentar identificar blocos de resultado.
            
            results_found = False
            
            # 1. Seletores Espec√≠ficos Conhecidos
            possible_result_selectors = ['div.g', 'div.MjjYud', 'div.tF2Cxc']
            
            for selector in possible_result_selectors:
                items = soup.select(selector)
                if items:
                    for item in items:
                        # Tenta extrair t√≠tulo (h3) e link (a)
                        h3 = item.find('h3')
                        a = item.find('a', href=True)
                        
                        if h3 and a:
                            link = a['href']
                            title = h3.get_text(strip=True)
                            
                            # Tenta extrair snippet (texto descritivo)
                            # Geralmente est√° em um div ou span ap√≥s o t√≠tulo
                            # Vamos pegar todo o texto do container e remover o t√≠tulo
                            container_text = item.get_text(separator=' ', strip=True)
                            snippet = container_text.replace(title, '', 1).strip()
                            # Limpeza extra do snippet (remover URLs visuais comuns)
                            snippet = snippet.replace(link, '')
                            
                            if link.startswith('http') and 'google.com' not in link:
                                results.append({
                                    "title": title,
                                    "link": link,
                                    "snippet": snippet[:400]
                                })
                                results_found = True
                    
                    if results_found:
                        break # Se funcionou com este seletor, paramos.

            # 2. Fallback Agressivo (Se seletores falharem)
            if not results_found:
                logger.warning("‚ö†Ô∏è Seletores espec√≠ficos falharam. Usando extra√ß√£o por proximidade.")
                # Procura todos h3 (que geralmente s√£o t√≠tulos)
                all_h3 = soup.find_all('h3')
                for h3 in all_h3:
                    # O link costuma ser o pai ou vizinho
                    parent_a = h3.find_parent('a', href=True)
                    if not parent_a:
                        # √Äs vezes o h3 est√° dentro do a, ou o a est√° logo antes
                        continue
                        
                    link = parent_a['href']
                    title = h3.get_text(strip=True)
                    
                    if not link.startswith('http') or 'google.com' in link:
                        continue

                    # Tenta pegar o snippet: texto no elemento pai do link (container do resultado)
                    container = parent_a.find_parent('div')
                    snippet = ""
                    if container:
                        full_text = container.get_text(separator=' ', strip=True)
                        snippet = full_text.replace(title, '', 1).replace(link, '').strip()
                    
                    results.append({
                        "title": title,
                        "link": link,
                        "snippet": snippet[:400]
                    })
                        
    except Exception as e:
        logger.error(f"‚ùå Erro na execu√ß√£o da busca: {e}")
        return []

    logger.info(f"‚úÖ Encontrados {len(results)} resultados na busca.")
    return results

async def find_company_website(
    razao_social: str, 
    nome_fantasia: str, 
    cnpj: str,
    email: Optional[str] = None,
    municipio: Optional[str] = None,
    cnaes: Optional[List[str]] = None
) -> Optional[str]:
    """
    Orquestra a descoberta do site oficial da empresa.
    1. Monta as queries (M√∫ltiplas buscas: Nome Fantasia e Raz√£o Social separadas)
    2. Busca no Google
    3. Usa LLM com contexto rico (Email, Cidade, CNAEs) para analisar resultados
    """
    
    queries = []
    
    nf = nome_fantasia.strip() if nome_fantasia else ""
    rs = razao_social.strip() if razao_social else ""
    city = municipio.strip() if municipio else ""
    
    # Query 1: Nome Fantasia + Municipio (se existir)
    if nf:
        q1 = f'{nf} {city} site oficial'.strip()
        queries.append(q1)
    
    # Query 2: Raz√£o Social + Municipio (se existir)
    if rs:
        # Remover "LTDA", "S.A.", "EIRELI", "ME", "EPP" para limpar a busca
        clean_rs = rs.replace(" LTDA", "").replace(" S.A.", "").replace(" EIRELI", "").replace(" ME", "").replace(" EPP", "")
        q2 = f'{clean_rs} {city} site oficial'.strip()
        queries.append(q2)
    
    # Query 3: Busca apenas pelo primeiro nome (Marca) + site oficial
    # Isso ajuda quando o nome fantasia √© longo (ECOMINERAL TECH LTDA) mas o site √© curto (ecomineral.com.br)
    if nf:
        first_name = nf.split()[0]
        if len(first_name) > 3: # Evitar siglas muito curtas isoladas
            q3 = f'{first_name} {city} site oficial'.strip()
            queries.append(q3)
            # Query 4: Nome curto sem cidade (para empresas nacionais)
            q4 = f'{first_name} site oficial'.strip()
            queries.append(q4)
    elif rs:
         # Fallback para primeiro nome da raz√£o social se n√£o tiver fantasia
        clean_rs = rs.replace(" LTDA", "").replace(" S.A.", "").replace(" EIRELI", "").replace(" ME", "").replace(" EPP", "")
        first_name = clean_rs.split()[0]
        if len(first_name) > 3:
             q3 = f'{first_name} {city} site oficial'.strip()
             queries.append(q3)
    
    # Se n√£o gerou queries (input vazio), retorna
    if not queries:
        logger.warning("‚ö†Ô∏è Sem Nome Fantasia ou Raz√£o Social para busca.")
        return None

    # ESTRAT√âGIA EXTRA: Valida√ß√£o de E-mail (Apenas Log)
    # Se tiver email corporativo, logamos para debug, mas n√£o for√ßamos busca espec√≠fica.
    if email and "@" in email:
        domain_part = email.split("@")[1].lower().strip()
        generic_domains = [
            "gmail.com", "outlook.com", "hotmail.com", "yahoo.com", "yahoo.com.br", 
            "uol.com.br", "bol.com.br", "terra.com.br", "ig.com.br", "icloud.com", "me.com"
        ]
        if domain_part not in generic_domains and "." in domain_part:
            logger.info(f"üìß Dom√≠nio de email dispon√≠vel para valida√ß√£o cruzada: {domain_part}")

    # Executar buscas (sequencial para evitar rate limit agressivo)
    all_results = []
    seen_links = set()
    
    # Executar queries em ordem de prioridade
    
    for q in queries:
        res = await search_google_serper(q)
        for r in res:
            if r['link'] not in seen_links:
                all_results.append(r)
                seen_links.add(r['link'])
    
    if not all_results:
        logger.warning("‚ö†Ô∏è Nenhum resultado encontrado no Google ap√≥s m√∫ltiplas buscas.")
        return None
    
    if not all_results:
        logger.warning("‚ö†Ô∏è Nenhum resultado encontrado no Google ap√≥s m√∫ltiplas buscas.")
        return None
        
    # Logar resultados consolidados para debug
    logger.info(f"üîç Resultados consolidados enviados para IA ({len(all_results)} itens):")
    logger.info(json.dumps(all_results, indent=2, ensure_ascii=False))

    # 3. Analisar com LLM
    client = AsyncOpenAI(api_key=settings.LLM_API_KEY, base_url=settings.LLM_BASE_URL)
    
    # Preparar input para o LLM
    results_text = json.dumps(all_results, indent=2, ensure_ascii=False)
    
    user_content = f"""
    Dados da Empresa:
    - Nome Fantasia: {nome_fantasia or 'N√£o informado'}
    - Raz√£o Social: {razao_social or 'N√£o informado'}
    - CNPJ: {cnpj or 'N√£o informado'}
    - E-mail: {email or 'N√£o informado'}
    - Munic√≠pio: {municipio or 'N√£o informado'}
    - CNAEs (Atividades): {', '.join(cnaes) if cnaes else 'N√£o informado'}
    
    Resultados da Busca (Consolidados):
    {results_text}
    """
    
    try:
        response = await client.chat.completions.create(
            model=settings.LLM_MODEL,
            messages=[
                {"role": "system", "content": DISCOVERY_PROMPT},
                {"role": "user", "content": user_content}
            ],
            temperature=0.0,
            response_format={"type": "json_object"}
        )
        
        content = response.choices[0].message.content.strip()
        logger.info(f"üß† Decis√£o do LLM: {content}")
        
        try:
            data = json.loads(content)
        except json.JSONDecodeError:
            # Tentar limpar markdown se houver
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
                data = json.loads(content)
            else:
                raise

        # Tratamento para caso a IA retorne uma lista em vez de um objeto
        if isinstance(data, list):
            if len(data) > 0:
                data = data[0]
            else:
                logger.warning("‚ö†Ô∏è IA retornou lista vazia.")
                return None
        
        if data.get("site_oficial") == "sim" and data.get("site") and data.get("site") != "nao_encontrado":
            return data.get("site")
        else:
            logger.info(f"‚ùå Site n√£o encontrado ou n√£o oficial. Justificativa: {data.get('justificativa')}")
            return None
            
    except Exception as e:
        logger.error(f"‚ùå Erro na an√°lise do LLM para descoberta de site: {e}")
        return None

