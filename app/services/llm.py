import json
import asyncio
import time
from typing import List, Dict, Any, Optional, Tuple
from openai import AsyncOpenAI, RateLimitError, APIError, APITimeoutError, BadRequestError, NotFoundError
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type, before_sleep_log
import logging
import json_repair
from app.core.config import settings
from app.schemas.profile import CompanyProfile

# Configurar logger
logger = logging.getLogger(__name__)

# Semaphores individuais por provedor LLM (respeitar rate limits)
llm_semaphores = {
    # Gemini: 10M tokens/min, 10k RPM. 
    # Usamos 15 concurrency seguro.
    "Google Gemini": asyncio.Semaphore(15),      
    
    # OpenAI: 4M tokens/min, 5k RPM.
    # Usamos 10 concurrency seguro.
    "OpenAI": asyncio.Semaphore(10),             
}

# Semaphore global para throttling geral (max 20 requisi√ß√µes simult√¢neas ao total)
# Limitado pelo servidor da aplica√ß√£o, n√£o apenas pelas APIs
llm_global_semaphore = asyncio.Semaphore(20)

# Configura√ß√£o de fallback chain
FALLBACK_CHAIN = [
    ("Google Gemini", settings.GOOGLE_API_KEY, settings.GOOGLE_BASE_URL, settings.GOOGLE_MODEL),
    ("OpenAI", settings.OPENAI_API_KEY, settings.OPENAI_BASE_URL, settings.OPENAI_MODEL),
]

# Filtrar apenas provedores com chave configurada
AVAILABLE_PROVIDERS = [(name, key, url, model) for name, key, url, model in FALLBACK_CHAIN if key]

if not AVAILABLE_PROVIDERS:
    logger.error("CRITICAL: Nenhum provedor de LLM configurado! Defina pelo menos uma API key.")

# Cliente prim√°rio (atual)
client_args = {
    "api_key": settings.LLM_API_KEY,
    "base_url": settings.LLM_BASE_URL,
}

client = AsyncOpenAI(**client_args)

SYSTEM_PROMPT = """Voc√™ √© um extrator de dados B2B especializado. Gere estritamente um JSON v√°lido correspondente ao schema abaixo.
Extraia dados do texto Markdown e PDF fornecido.

INSTRU√á√ïES CR√çTICAS:
1. IDIOMA DE SA√çDA: PORTUGU√äS (BRASIL). Todo o conte√∫do extra√≠do deve estar em Portugu√™s. Traduza descri√ß√µes, cargos e categorias. Mantenha em ingl√™s apenas termos t√©cnicos globais (ex: "SaaS", "Big Data", "Machine Learning") ou nomes pr√≥prios de produtos n√£o traduz√≠veis.
2. PRODUTOS vs SERVI√áOS: Distinga claramente entre produtos f√≠sicos e servi√ßos intang√≠veis.
3. DETALHES DO SERVI√áO: Para os principais servi√ßos, tente extrair 'metodologia' (como eles fazem) e 'entreg√°veis' (o que o cliente recebe).
4. LISTAGEM DE PRODUTOS EXAUSTIVA - CR√çTICO E OBRIGAT√ìRIO: 
   - Ao extrair 'product_categories', voc√™ DEVE preencher o campo 'items' de CADA categoria com TODOS os produtos individuais encontrados.
   - NUNCA deixe 'items' vazio ou como array vazio []. Se uma categoria √© mencionada, voc√™ DEVE encontrar e listar os produtos espec√≠ficos.
   - O QUE S√ÉO ITEMS: Items s√£o PRODUTOS ESPEC√çFICOS (nomes de produtos, modelos, refer√™ncias, SKUs). N√ÉO s√£o nomes de categorias, N√ÉO s√£o marcas isoladas, N√ÉO s√£o descri√ß√µes gen√©ricas de categorias.
   - EXEMPLO CORRETO: Se o texto menciona "Fios e Cabos" e lista "Cabo 1KV HEPR", "Cabo 1KV LSZH", "Cabo Flex 750V", ent√£o 'items' DEVE ser ["Cabo 1KV HEPR", "Cabo 1KV LSZH", "Cabo Flex 750V"].
   - EXEMPLO INCORRETO: N√ÉO fa√ßa {"category_name": "Fios e Cabos", "items": ["Fios e Cabos", "Automa√ß√£o"]} - esses s√£o nomes de categorias, n√£o produtos.
   - EXEMPLO INCORRETO: N√ÉO fa√ßa {"category_name": "Marcas", "items": ["Philips", "Siemens"]} - marcas isoladas n√£o s√£o produtos. Se houver "Lumin√°ria Philips XYZ", extraia "Lumin√°ria Philips XYZ" como item.
   - PROCURE no texto: nomes de produtos, modelos, refer√™ncias, SKUs, c√≥digos de produto, listas de itens, cat√°logos, especifica√ß√µes t√©cnicas.
   - Se voc√™ criar uma categoria, voc√™ DEVE preencher seus items com produtos encontrados no texto. Se n√£o encontrar produtos espec√≠ficos, N√ÉO crie a categoria.
   - N√ÉO crie categorias gen√©ricas como "Outras Categorias", "Marcas", "Geral" - apenas categorias espec√≠ficas mencionadas no conte√∫do.
   - Extraia TUDO que encontrar: nomes completos de produtos, modelos, marcas quando parte do nome do produto, refer√™ncias. N√ÉO resuma, N√ÉO filtre por "qualidade".
5. PROVA SOCIAL: Extraia Estudos de Caso espec√≠ficos, Nomes de Clientes e Certifica√ß√µes. Estes s√£o de alta prioridade.
6. ENGAJAMENTO: Procure como eles vendem (Mensalidade? Por Projeto? Aloca√ß√£o de equipe?).
7. CONSOLIDA√á√ÉO: Se receber m√∫ltiplos fragmentos de conte√∫do, consolide as informa√ß√µes sem duplicar. Priorize informa√ß√µes mais detalhadas e completas.

Se um campo n√£o for encontrado, use null ou lista vazia. N√ÉO gere blocos de c√≥digo markdown (```json). Gere APENAS a string JSON bruta.

Schema (Mantenha as chaves em ingl√™s, valores em Portugu√™s):
{
  "identity": { 
    "company_name": "string", 
    "cnpj": "string",
    "tagline": "string", 
    "description": "string", 
    "founding_year": "string",
    "employee_count_range": "string"
  },
  "classification": { 
    "industry": "string", 
    "business_model": "string", 
    "target_audience": "string",
    "geographic_coverage": "string"
  },
  "team": {
    "size_range": "string",
    "key_roles": ["string"],
    "team_certifications": ["string"]
  },
  "offerings": { 
    "products": ["string"],
    "product_categories": [
        { "category_name": "string", "items": ["string"] }
    ],
    "services": ["string"], 
    "service_details": [
        { 
          "name": "string", 
          "description": "string", 
          "methodology": "string", 
          "deliverables": ["string"],
          "ideal_client_profile": "string"
        }
    ],
    "engagement_models": ["string"],
    "key_differentiators": ["string"] 
  },
  "reputation": {
    "certifications": ["string"],
    "awards": ["string"],
    "partnerships": ["string"],
    "client_list": ["string"],
    "case_studies": [
        {
          "title": "string",
          "client_name": "string",
          "industry": "string",
          "challenge": "string",
          "solution": "string",
          "outcome": "string"
        }
    ]
  },
  "contact": { 
    "emails": ["string"], 
    "phones": ["string"], 
    "linkedin_url": "string", 
    "website_url": "string",
    "headquarters_address": "string",
    "locations": ["string"]
  }
}
"""

# --- UTILS ---

def estimate_tokens(text: str, include_overhead: bool = True) -> int:
    """
    Estima a quantidade de tokens em um texto.
    Aproxima√ß√£o melhorada para portugu√™s e conte√∫do HTML/Markdown:
    - 1 token ‚âà 2.5 caracteres (mais conservador que 4)
    - include_overhead: Se True, adiciona overhead do prompt do sistema (~50k tokens)
    """
    base_tokens = len(text) // 2.5  # Melhor para portugu√™s
    
    if include_overhead:
        system_prompt_tokens = 50000  # Overhead do SYSTEM_PROMPT
        return int(base_tokens + system_prompt_tokens)
    
    return int(base_tokens)

def chunk_content(text: str, max_tokens: int = 500_000) -> List[str]:
    """
    Divide o conte√∫do em chunks respeitando o limite de tokens.
    NOVA ESTRAT√âGIA OTIMIZADA: Agrupamento Inteligente (Smart Chunking).
    - Agrupa m√∫ltiplas p√°ginas pequenas em um √∫nico chunk para reduzir overhead de requisi√ß√µes.
    - Mant√©m p√°ginas processadas isoladamente se forem muito grandes.
    - S√≥ divide uma p√°gina se ela exceder o limite de tokens
    
    max_tokens padr√£o: 500k (50% do limite do Gemini de 1.048M)
    """
    # Separar por marcadores de p√°gina
    page_markers = text.split("--- PAGE START:")
    raw_pages = []
    
    for i, page in enumerate(page_markers):
        if i == 0 and not page.strip():
            continue  # Pular chunk vazio inicial
        
        # Construir conte√∫do da p√°gina com marcador
        page_content = "--- PAGE START:" + page if i > 0 else page
        page_tokens = estimate_tokens(page_content)
        page_chars = len(page_content)
        
        # Se a p√°gina individual excede o limite, ela deve ser tratada separadamente (e dividida se necess√°rio)
        if page_tokens > max_tokens:
            logger.warning(f"‚ö†Ô∏è P√°gina {i+1} muito grande ({page_tokens:,} tokens, {page_chars:,} chars), dividindo em partes...")
            page_chunks = _split_large_page(page_content, max_tokens)
            raw_pages.extend(page_chunks)
            logger.info(f"  üìÑ P√°gina {i+1} dividida em {len(page_chunks)} partes")
        else:
            raw_pages.append(page_content)
            
    # Agrupar p√°ginas em chunks maiores
    # Alvo: ~20k tokens por chunk (balanceado para evitar "Lost in the Middle")
    # Reduzido de 100k para 20k para garantir que o modelo capture detalhes de todos os itens
    GROUP_TARGET_TOKENS = 20_000
    
    grouped_chunks = []
    current_group = ""
    current_tokens = 0
    
    logger.info(f"Agrupando {len(raw_pages)} p√°ginas em chunks (Alvo: {GROUP_TARGET_TOKENS} tokens)...")
    
    for page in raw_pages:
        # Usar contagem de tokens SEM overhead para agrupar conte√∫do
        # O overhead do system prompt ser√° adicionado apenas uma vez por chunk final
        page_tokens = estimate_tokens(page, include_overhead=False)
        
        # Se adicionar a p√°gina atual ultrapassa o alvo E j√° temos conte√∫do no grupo...
        # (Se o grupo est√° vazio, aceitamos a p√°gina mesmo que seja grande, desde que < max_tokens global)
        if current_tokens + page_tokens > GROUP_TARGET_TOKENS and current_group:
            grouped_chunks.append(current_group)
            current_group = page
            current_tokens = page_tokens
        else:
            if current_group:
                current_group += "\n\n" + page
            else:
                current_group = page
            current_tokens += page_tokens
            
    if current_group:
        grouped_chunks.append(current_group)
    
    logger.info(f"‚úÖ Conte√∫do consolidado em {len(grouped_chunks)} chunks (era {len(raw_pages)} p√°ginas)")
    return grouped_chunks

def _split_large_page(page_content: str, max_tokens: int) -> List[str]:
    """
    Divide uma p√°gina muito grande em m√∫ltiplos chunks menores.
    Tenta dividir por par√°grafos ou linhas para manter contexto.
    Usa margem de seguran√ßa de 80% do max_tokens para evitar exceder limites.
    """
    # Margem de seguran√ßa: usar 80% do limite para garantir que n√£o exceda
    safe_max_tokens = int(max_tokens * 0.8)
    chunks = []
    current_chunk = ""
    current_tokens = 0
    
    # Tentar dividir por par√°grafos duplos primeiro (melhor contexto)
    paragraphs = page_content.split('\n\n')
    
    # Se n√£o houver par√°grafos duplos, dividir por linhas
    if len(paragraphs) == 1:
        paragraphs = page_content.split('\n')
    
    for para in paragraphs:
        para_with_sep = para + ('\n\n' if '\n\n' in page_content else '\n')
        para_tokens = estimate_tokens(para_with_sep)
        
        # Se um par√°grafo individual excede o limite, dividir por linhas
        if para_tokens > safe_max_tokens:
            logger.warning(f"‚ö†Ô∏è Par√°grafo muito grande ({para_tokens} tokens), dividindo por linhas...")
            para_lines = para.split('\n')
            for line in para_lines:
                line_with_newline = line + '\n'
                line_tokens = estimate_tokens(line_with_newline)
                
                # Se uma linha excede o limite, truncar
                if line_tokens > safe_max_tokens:
                    logger.warning(f"‚ö†Ô∏è Linha muito grande ({line_tokens} tokens), truncando...")
                    max_chars = int(safe_max_tokens * 2.5)  # 2.5 chars por token
                    truncated = line[:max_chars]
                    if current_chunk:
                        chunks.append(current_chunk)
                        current_chunk = ""
                        current_tokens = 0
                    chunks.append(truncated)
                    continue
                
                # Se adicionar esta linha exceder o limite, finalizar chunk atual
                if current_tokens + line_tokens > safe_max_tokens:
                    if current_chunk:
                        chunks.append(current_chunk)
                    current_chunk = line_with_newline
                    current_tokens = line_tokens
                else:
                    current_chunk += line_with_newline
                    current_tokens += line_tokens
            continue
        
        # Se adicionar este par√°grafo exceder o limite, finalizar chunk atual
        if current_tokens + para_tokens > safe_max_tokens:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = para_with_sep
            current_tokens = para_tokens
        else:
            current_chunk += para_with_sep
            current_tokens += para_tokens
    
    # Adicionar √∫ltimo chunk
    if current_chunk:
        chunks.append(current_chunk)
    
    # Validar que todos os chunks est√£o dentro do limite
    for i, chunk in enumerate(chunks):
        chunk_tokens = estimate_tokens(chunk)
        if chunk_tokens > max_tokens:
            logger.warning(f"‚ö†Ô∏è Chunk {i+1} ainda excede limite ({chunk_tokens} > {max_tokens} tokens), truncando...")
            max_chars = int(max_tokens * 2.5)
            chunks[i] = chunk[:max_chars]
    
    return chunks

def merge_profiles(profiles: List[CompanyProfile]) -> CompanyProfile:
    """
    Consolida m√∫ltiplos perfis parciais em um √∫nico perfil completo.
    Prioriza informa√ß√µes mais completas e remove duplicatas.
    Remove profiles None antes de processar.
    """
    logger.info(f"üîÑ Iniciando merge de {len(profiles)} perfis")
    
    # Filtrar profiles None ou inv√°lidos
    valid_profiles = [p for p in profiles if p is not None and isinstance(p, CompanyProfile)]
    invalid_count = len(profiles) - len(valid_profiles)
    
    if invalid_count > 0:
        logger.warning(f"‚ö†Ô∏è {invalid_count} perfis inv√°lidos/None foram filtrados")
    
    if not valid_profiles:
        logger.warning("‚ùå Nenhum profile v√°lido para mergear, retornando perfil vazio")
        return CompanyProfile()
    
    if len(valid_profiles) == 1:
        logger.info("‚ÑπÔ∏è Apenas 1 perfil v√°lido, retornando sem merge")
        return valid_profiles[0]
    
    # Analisar dados antes do merge
    logger.info(f"üìä Analisando {len(valid_profiles)} perfis v√°lidos antes do merge:")
    for i, profile in enumerate(valid_profiles):
        p_dict = profile.model_dump()
        filled_fields = sum(1 for k, v in p_dict.items() 
                          if v and (isinstance(v, dict) and any(v.values()) or isinstance(v, list) and len(v) > 0))
        logger.info(f"  Perfil {i+1}: {filled_fields} campos preenchidos")
        if filled_fields > 0:
            # Mostrar quais campos t√™m dados
            for key, value in p_dict.items():
                if value and (isinstance(value, dict) and any(v for v in value.values() if v) or isinstance(value, list) and len(value) > 0):
                    logger.debug(f"    - {key}: {len(value) if isinstance(value, list) else 'objeto com dados'}")
    
    # Escolher perfil mais completo como base
    # IMPORTANTE: Todos os perfis ser√£o mergeados depois, ent√£o a escolha do base
    # apenas determina qual ser√° o ponto de partida. N√£o perdemos informa√ß√µes.
    def count_filled_fields(profile_dict: dict) -> int:
        """Conta quantos campos t√™m dados preenchidos"""
        count = 0
        for key, value in profile_dict.items():
            if value and (isinstance(value, dict) and any(v for v in value.values() if v) or isinstance(value, list) and len(value) > 0):
                count += 1
        return count
    
    def score_profile_completeness(profile_dict: dict) -> int:
        """Score mais sofisticado: conta campos + itens em listas + comprimento de textos"""
        score = 0
        for key, value in profile_dict.items():
            if isinstance(value, dict):
                for sub_key, sub_value in value.items():
                    if sub_value:
                        if isinstance(sub_value, list):
                            score += len(sub_value)  # Mais itens = mais completo
                        elif isinstance(sub_value, str):
                            score += len(sub_value) // 10  # Textos maiores = mais completo
                        else:
                            score += 1
            elif isinstance(value, list) and len(value) > 0:
                score += len(value)
            elif value:
                score += 1
        return score
    
    # Encontrar perfil mais completo usando score sofisticado
    profiles_dicts = [p.model_dump() for p in valid_profiles]
    base_idx = max(range(len(profiles_dicts)), key=lambda i: score_profile_completeness(profiles_dicts[i]))
    merged = profiles_dicts[base_idx].copy()
    base_score = score_profile_completeness(merged)
    logger.info(f"üìå Usando perfil {base_idx+1} como base (score de completude: {base_score})")
    
    # Mergear outros perfis
    for i, profile in enumerate(valid_profiles):
        if i == base_idx:
            continue  # Pular o perfil base
        
        p_dict = profile.model_dump()
        profile_num = i + 1
        logger.debug(f"üîÑ Mergeando perfil {profile_num}/{len(valid_profiles)}")
        
        # Mergear campos de texto de forma inteligente
        # CR√çTICO: Descri√ß√µes podem come√ßar em um chunk e terminar em outro
        # Precisamos detectar se s√£o complementares e concatenar, n√£o apenas escolher uma
        def are_texts_complementary(text1: str, text2: str, similarity_threshold: float = 0.3) -> bool:
            """
            Detecta se dois textos s√£o complementares (n√£o duplicados).
            Se houver sobreposi√ß√£o significativa, s√£o duplicados.
            Se forem muito diferentes, podem ser complementares.
            """
            if not text1 or not text2:
                return False
            
            text1_lower = text1.lower().strip()
            text2_lower = text2.lower().strip()
            
            # Se um est√° contido no outro, s√£o duplicados
            if text1_lower in text2_lower or text2_lower in text1_lower:
                return False
            
            # Calcular similaridade simples (palavras em comum)
            words1 = set(text1_lower.split())
            words2 = set(text2_lower.split())
            if len(words1) == 0 or len(words2) == 0:
                return False
            
            common_words = words1 & words2
            similarity = len(common_words) / max(len(words1), len(words2))
            
            # Se similaridade < threshold, s√£o complementares
            return similarity < similarity_threshold
        
        def merge_text_fields(current: Optional[str], new: Optional[str], field_name: str) -> str:
            """
            Merge inteligente de campos de texto:
            1. Se current vazio, usar new
            2. Se new vazio, manter current
            3. Se s√£o complementares, concatenar
            4. Se s√£o duplicados ou similares, usar o mais longo/completo
            """
            if not new:
                return current or ""
            if not current:
                return new
            
            # Campos que podem ser concatenados (descri√ß√µes, metodologias)
            concatenatable_fields = ["description", "methodology", "tagline"]
            
            if field_name in concatenatable_fields:
                if are_texts_complementary(current, new):
                    # Textos complementares: concatenar
                    merged_text = f"{current.strip()}. {new.strip()}"
                    logger.debug(f"  üîó Concatenado campo '{field_name}': textos complementares detectados")
                    return merged_text
                else:
                    # Textos similares/duplicados: usar o mais longo
                    if len(new) > len(current):
                        logger.debug(f"  üìù Substitu√≠do campo '{field_name}': novo texto √© mais longo ({len(new)} vs {len(current)} chars)")
                        return new
                    else:
                        return current
            else:
                # Campos n√£o-concaten√°veis: usar o mais longo ou mais completo
                if len(new) > len(current):
                    return new
                return current
        
        # Mergear campos simples com merge inteligente
        for section in ["identity", "classification", "team", "contact"]:
            if section in merged and section in p_dict:
                for key, value in p_dict[section].items():
                    if not value:
                        continue
                    
                    current_value = merged[section].get(key)
                    field_path = f"{section}.{key}"
                    
                    if isinstance(value, str) and isinstance(current_value, str):
                        # Campo de texto: usar merge inteligente
                        merged[section][key] = merge_text_fields(current_value, value, key)
                    elif value and not current_value:
                        # Campo n√£o-texto: usar se current vazio
                        merged[section][key] = value
                    elif isinstance(value, str) and len(value) > len(str(current_value or "")):
                        # Campo texto novo √© mais longo
                        merged[section][key] = value
        
        # Mergear listas (uni√£o sem duplicatas)
        if "offerings" in merged and "offerings" in p_dict:
            merged["offerings"]["products"] = list(set(merged["offerings"].get("products", []) + p_dict["offerings"].get("products", [])))
            merged["offerings"]["services"] = list(set(merged["offerings"].get("services", []) + p_dict["offerings"].get("services", [])))
            merged["offerings"]["engagement_models"] = list(set(merged["offerings"].get("engagement_models", []) + p_dict["offerings"].get("engagement_models", [])))
            merged["offerings"]["key_differentiators"] = list(set(merged["offerings"].get("key_differentiators", []) + p_dict["offerings"].get("key_differentiators", [])))
            
            # Mergear service_details: se servi√ßo j√° existe, fazer merge dos campos
            service_dict = {s["name"]: s for s in merged["offerings"].get("service_details", [])}
            
            for service in p_dict["offerings"].get("service_details", []):
                service_name = service.get("name")
                if not service_name or not isinstance(service_name, str):
                    logger.warning(f"  ‚ö†Ô∏è Servi√ßo sem nome v√°lido ignorado: {service}")
                    continue
                
                if service_name in service_dict:
                    # Servi√ßo j√° existe: fazer merge inteligente dos campos
                    existing = service_dict[service_name]
                    # Mergear description (usar merge inteligente - pode concatenar se complementares)
                    existing["description"] = merge_text_fields(
                        existing.get("description"), 
                        service.get("description"), 
                        "description"
                    )
                    # Mergear methodology (usar merge inteligente)
                    existing["methodology"] = merge_text_fields(
                        existing.get("methodology"), 
                        service.get("methodology"), 
                        "methodology"
                    )
                    # Mergear deliverables (uni√£o sem duplicatas)
                    existing_deliverables = set(existing.get("deliverables", []))
                    new_deliverables = set(service.get("deliverables", []))
                    existing["deliverables"] = list(existing_deliverables | new_deliverables)
                    # Mergear ideal_client_profile (usar merge inteligente)
                    existing["ideal_client_profile"] = merge_text_fields(
                        existing.get("ideal_client_profile"), 
                        service.get("ideal_client_profile"), 
                        "ideal_client_profile"
                    )
                    logger.debug(f"  üîÑ Mergeado servi√ßo '{service_name}': {len(existing.get('deliverables', []))} deliverables")
                else:
                    # Novo servi√ßo: adicionar
                    service_dict[service_name] = service.copy()
                    logger.debug(f"  ‚ûï Novo servi√ßo adicionado: '{service_name}'")
            
            # Atualizar merged com os servi√ßos modificados
            merged["offerings"]["service_details"] = list(service_dict.values())
            
            # Mergear product_categories: se categoria j√° existe, fazer merge dos items
            # Criar dict indexado por nome para acesso r√°pido
            cat_dict = {c["category_name"]: c for c in merged["offerings"].get("product_categories", [])}
            
            for cat in p_dict["offerings"].get("product_categories", []):
                cat_name = cat.get("category_name")
                if not cat_name or not isinstance(cat_name, str):
                    logger.warning(f"  ‚ö†Ô∏è Categoria sem nome v√°lido ignorada: {cat}")
                    continue
                
                if cat_name in cat_dict:
                    # Categoria j√° existe: fazer merge dos items (uni√£o sem duplicatas)
                    existing_items = set(cat_dict[cat_name].get("items", []))
                    new_items = set(cat.get("items", []))
                    merged_items = list(existing_items | new_items)  # Uni√£o de sets
                    cat_dict[cat_name]["items"] = merged_items
                    logger.debug(f"  üîÑ Mergeado items da categoria '{cat_name}': {len(existing_items)} + {len(new_items)} = {len(merged_items)} items")
                else:
                    # Nova categoria: adicionar
                    cat_dict[cat_name] = cat.copy()  # Fazer c√≥pia para n√£o modificar original
                    logger.debug(f"  ‚ûï Nova categoria adicionada: '{cat_name}' com {len(cat.get('items', []))} items")
            
            # Atualizar merged com as categorias modificadas
            merged["offerings"]["product_categories"] = list(cat_dict.values())
        
        # Mergear reputation
        if "reputation" in merged and "reputation" in p_dict:
            merged["reputation"]["certifications"] = list(set(merged["reputation"].get("certifications", []) + p_dict["reputation"].get("certifications", [])))
            merged["reputation"]["awards"] = list(set(merged["reputation"].get("awards", []) + p_dict["reputation"].get("awards", [])))
            merged["reputation"]["partnerships"] = list(set(merged["reputation"].get("partnerships", []) + p_dict["reputation"].get("partnerships", [])))
            merged["reputation"]["client_list"] = list(set(merged["reputation"].get("client_list", []) + p_dict["reputation"].get("client_list", [])))
            
            # Mergear case studies: se case study j√° existe, fazer merge dos campos
            case_dict = {cs["title"]: cs for cs in merged["reputation"].get("case_studies", [])}
            
            for case in p_dict["reputation"].get("case_studies", []):
                case_title = case.get("title")
                if not case_title or not isinstance(case_title, str):
                    logger.warning(f"  ‚ö†Ô∏è Case study sem t√≠tulo v√°lido ignorado: {case}")
                    continue
                
                if case_title in case_dict:
                    # Case study j√° existe: fazer merge inteligente dos campos
                    existing = case_dict[case_title]
                    # Campos de texto que podem ser concatenados
                    text_fields = ["challenge", "solution", "outcome"]
                    # Campos simples (n√£o concaten√°veis)
                    simple_fields = ["client_name", "industry"]
                    
                    # Mergear campos de texto com merge inteligente
                    for field in text_fields:
                        if case.get(field):
                            existing[field] = merge_text_fields(
                                existing.get(field), 
                                case.get(field), 
                                field
                            )
                    
                    # Mergear campos simples (usar o mais longo se ambos existirem)
                    for field in simple_fields:
                        if case.get(field) and (not existing.get(field) or len(str(case[field])) > len(str(existing.get(field, "")))):
                            existing[field] = case[field]
                    
                    logger.debug(f"  üîÑ Mergeado case study '{case_title}'")
                else:
                    # Novo case study: adicionar
                    case_dict[case_title] = case.copy()
                    logger.debug(f"  ‚ûï Novo case study adicionado: '{case_title}'")
            
            # Atualizar merged com os case studies modificados
            merged["reputation"]["case_studies"] = list(case_dict.values())
        
        # Mergear sources (uni√£o sem duplicatas, preservando ordem)
        if "sources" in merged and "sources" in p_dict:
            existing_sources = set(merged.get("sources", []))
            new_sources = set(p_dict.get("sources", []))
            # Preservar ordem: primeiro os existentes, depois os novos
            merged["sources"] = list(merged.get("sources", [])) + [s for s in p_dict.get("sources", []) if s not in existing_sources]
    
    # Valida√ß√£o e limpeza final antes de criar CompanyProfile
    # Garantir que todas as listas s√£o v√°lidas
    if "offerings" in merged and isinstance(merged["offerings"], dict):
        offerings = merged["offerings"]
        # Remover strings vazias de listas
        for field in ["products", "services", "engagement_models", "key_differentiators"]:
            if isinstance(offerings.get(field), list):
                offerings[field] = [item for item in offerings[field] if isinstance(item, str) and item.strip()]
        
        # Validar product_categories: remover apenas categorias que s√£o claramente metadados/erros estruturais
        # MANTER categorias sem items - se o fornecedor mencionou a categoria, √© informa√ß√£o v√°lida
        # N√ÉO filtrar items por "qualidade" - se foi extra√≠do, deve ser mantido
        if isinstance(offerings.get("product_categories"), list):
            valid_cats = []
            invalid_structure_cats = []
            
            # Categorias que s√£o claramente metadados/erros estruturais (n√£o categorias reais de produtos)
            invalid_category_names = {
                "outras categorias", "outras", "marcas", "marca", "geral", "diversos", 
                "outros", "categorias", "categoria", "produtos", "produto"
            }
            
            for cat in offerings["product_categories"]:
                if not isinstance(cat, dict) or not cat.get("category_name"):
                    continue
                
                cat_name = cat.get("category_name", "").strip().lower()
                
                # Remover apenas categorias que s√£o claramente metadados/erros estruturais
                if cat_name in invalid_category_names:
                    invalid_structure_cats.append(cat.get("category_name"))
                    logger.debug(f"  üóëÔ∏è Categoria inv√°lida (metadado) removida: '{cat.get('category_name')}'")
                    continue
                
                # Garantir que items √© uma lista v√°lida (mesmo que vazia)
                if not isinstance(cat.get("items"), list):
                    cat["items"] = []
                else:
                    # Filtrar apenas strings vazias (n√£o filtrar por "qualidade")
                    cat["items"] = [item for item in cat["items"] if isinstance(item, str) and item.strip()]
                
                # MANTER a categoria mesmo se items estiver vazio - √© informa√ß√£o v√°lida do fornecedor
                valid_cats.append(cat)
            
            if invalid_structure_cats:
                logger.info(f"üóëÔ∏è {len(invalid_structure_cats)} categorias inv√°lidas (metadados) removidas: {invalid_structure_cats}")
            offerings["product_categories"] = valid_cats
        
        # Validar service_details
        if isinstance(offerings.get("service_details"), list):
            valid_services = []
            for service in offerings["service_details"]:
                if isinstance(service, dict) and service.get("name"):
                    if isinstance(service.get("deliverables"), list):
                        service["deliverables"] = [d for d in service["deliverables"] if isinstance(d, str) and d.strip()]
                    valid_services.append(service)
            offerings["service_details"] = valid_services
    
    # Validar reputation
    if "reputation" in merged and isinstance(merged["reputation"], dict):
        reputation = merged["reputation"]
        for field in ["certifications", "awards", "partnerships", "client_list"]:
            if isinstance(reputation.get(field), list):
                reputation[field] = [item for item in reputation[field] if isinstance(item, str) and item.strip()]
        
        # Validar case_studies
        if isinstance(reputation.get("case_studies"), list):
            valid_cases = []
            for case in reputation["case_studies"]:
                if isinstance(case, dict) and case.get("title"):
                    valid_cases.append(case)
            reputation["case_studies"] = valid_cases
    
    # Validar contact
    if "contact" in merged and isinstance(merged["contact"], dict):
        contact = merged["contact"]
        for field in ["emails", "phones", "locations"]:
            if isinstance(contact.get(field), list):
                contact[field] = [item for item in contact[field] if isinstance(item, str) and item.strip()]
    
    # Validar sources
    if isinstance(merged.get("sources"), list):
        merged["sources"] = [s for s in merged["sources"] if isinstance(s, str) and s.strip()]
    
    # Analisar resultado final do merge
    filled_fields = sum(1 for k, v in merged.items() 
                      if v and (isinstance(v, dict) and any(v.values()) or isinstance(v, list) and len(v) > 0))
    logger.info(f"‚úÖ Merge conclu√≠do: {filled_fields} campos preenchidos no perfil final")
    
    # Estat√≠sticas detalhadas
    if "offerings" in merged and isinstance(merged["offerings"], dict):
        offerings = merged["offerings"]
        total_products = len(offerings.get("products", []))
        total_categories = len(offerings.get("product_categories", []))
        categories_with_items = sum(1 for cat in offerings.get("product_categories", []) if cat.get("items"))
        total_items = sum(len(cat.get("items", [])) for cat in offerings.get("product_categories", []))
        logger.info(f"üì¶ Offerings: {total_products} produtos, {total_categories} categorias ({categories_with_items} com items, {total_items} items totais)")
    
    if filled_fields == 0:
        logger.warning("‚ö†Ô∏è ATEN√á√ÉO: Perfil final est√° completamente vazio ap√≥s merge!")
        logger.debug(f"üìã Estrutura do perfil final: {json.dumps(merged, indent=2, ensure_ascii=False)[:1000]}")
    else:
        # Mostrar quais campos t√™m dados
        for key, value in merged.items():
            if value and (isinstance(value, dict) and any(v for v in value.values() if v) or isinstance(value, list) and len(value) > 0):
                logger.info(f"  ‚úÖ {key}: {len(value) if isinstance(value, list) else 'objeto com dados'}")
    
    try:
        return CompanyProfile(**merged)
    except Exception as e:
        logger.error(f"‚ùå Erro ao criar CompanyProfile ap√≥s merge: {e}")
        logger.error(f"üìã Dados problem√°ticos: {json.dumps(merged, indent=2, ensure_ascii=False)[:2000]}")
        raise e

# --- CORE FUNCTIONS ---

@retry(
    retry=retry_if_exception_type((RateLimitError, APIError, APITimeoutError)),
    wait=wait_exponential(multiplier=1, min=2, max=120),
    stop=stop_after_attempt(3),
    before_sleep=before_sleep_log(logger, logging.WARNING)
)
def normalize_llm_response(data: Any) -> dict:
    """
    Normaliza e valida a resposta do LLM para garantir compatibilidade com CompanyProfile.
    Corrige:
    - Arrays retornados ao inv√©s de objetos
    - Campos None que deveriam ser listas vazias
    - Objetos aninhados com valores None
    """
    # Validar se √© um objeto (n√£o array)
    if isinstance(data, list):
        logger.warning(f"LLM retornou array ao inv√©s de objeto. Tentando extrair primeiro item...")
        if len(data) > 0 and isinstance(data[0], dict):
            data = data[0]
            logger.info("‚úÖ Array convertido para objeto (primeiro item extra√≠do)")
        else:
            logger.error(f"‚ùå Array vazio ou inv√°lido. Tipo do primeiro item: {type(data[0]) if len(data) > 0 else 'N/A'}")
            raise ValueError("LLM retornou array vazio ou inv√°lido")
    
    if not isinstance(data, dict):
        logger.error(f"‚ùå Tipo inv√°lido recebido: {type(data)}. Esperado: dict")
        raise ValueError(f"LLM retornou tipo inv√°lido: {type(data)}. Esperado dict, recebido {type(data).__name__}")
    
    # Garantir que campos de lista nunca sejam None
    # 1. TeamProfile
    if "team" in data:
        if not isinstance(data["team"], dict):
            data["team"] = {}
        team = data["team"]
        if team.get("key_roles") is None:
            team["key_roles"] = []
        if team.get("team_certifications") is None:
            team["team_certifications"] = []
    
    # 2. Offerings
    if "offerings" in data:
        if not isinstance(data["offerings"], dict):
            data["offerings"] = {}
        offerings = data["offerings"]
        
        # Listas simples
        for field in ["products", "services", "engagement_models", "key_differentiators"]:
            if offerings.get(field) is None:
                offerings[field] = []
            elif not isinstance(offerings[field], list):
                logger.warning(f"‚ö†Ô∏è Campo '{field}' n√£o √© uma lista, convertendo...")
                offerings[field] = []
        
        # Listas de objetos
        if offerings.get("product_categories") is None:
            offerings["product_categories"] = []
        elif not isinstance(offerings["product_categories"], list):
            logger.warning(f"‚ö†Ô∏è product_categories n√£o √© uma lista, convertendo...")
            offerings["product_categories"] = []
        else:
            # Validar e limpar cada ProductCategory
            valid_categories = []
            for cat in offerings["product_categories"]:
                if not isinstance(cat, dict):
                    logger.warning(f"‚ö†Ô∏è Categoria inv√°lida (n√£o √© dict): {cat}")
                    continue
                cat_name = cat.get("category_name")
                if not cat_name or not isinstance(cat_name, str):
                    logger.warning(f"‚ö†Ô∏è Categoria sem nome v√°lido ignorada: {cat}")
                    continue
                # Garantir que items √© uma lista de strings
                if cat.get("items") is None:
                    cat["items"] = []
                elif not isinstance(cat["items"], list):
                    logger.warning(f"‚ö†Ô∏è Items da categoria '{cat_name}' n√£o √© uma lista, convertendo...")
                    cat["items"] = []
                else:
                    # Filtrar apenas strings v√°lidas
                    cat["items"] = [item for item in cat["items"] if isinstance(item, str) and item.strip()]
                valid_categories.append(cat)
            offerings["product_categories"] = valid_categories
        
        if offerings.get("service_details") is None:
            offerings["service_details"] = []
        elif not isinstance(offerings["service_details"], list):
            logger.warning(f"‚ö†Ô∏è service_details n√£o √© uma lista, convertendo...")
            offerings["service_details"] = []
        else:
            # Validar cada ServiceDetail
            valid_services = []
            for service in offerings["service_details"]:
                if not isinstance(service, dict):
                    logger.warning(f"‚ö†Ô∏è Servi√ßo inv√°lido (n√£o √© dict): {service}")
                    continue
                if not service.get("name") or not isinstance(service.get("name"), str):
                    logger.warning(f"‚ö†Ô∏è Servi√ßo sem nome v√°lido ignorado: {service}")
                    continue
                if service.get("deliverables") is None:
                    service["deliverables"] = []
                elif not isinstance(service["deliverables"], list):
                    logger.warning(f"‚ö†Ô∏è Deliverables do servi√ßo '{service.get('name')}' n√£o √© uma lista, convertendo...")
                    service["deliverables"] = []
                else:
                    # Filtrar apenas strings v√°lidas
                    service["deliverables"] = [d for d in service["deliverables"] if isinstance(d, str) and d.strip()]
                valid_services.append(service)
            offerings["service_details"] = valid_services
    
    # 3. Reputation
    if "reputation" in data:
        if not isinstance(data["reputation"], dict):
            data["reputation"] = {}
        reputation = data["reputation"]
        
        # Listas simples
        for field in ["certifications", "awards", "partnerships", "client_list"]:
            if reputation.get(field) is None:
                reputation[field] = []
        
        # Lista de CaseStudies
        if reputation.get("case_studies") is None:
            reputation["case_studies"] = []
    
    # 4. Contact
    if "contact" in data:
        if not isinstance(data["contact"], dict):
            data["contact"] = {}
        contact = data["contact"]
        for field in ["emails", "phones", "locations"]:
            if contact.get(field) is None:
                contact[field] = []
    
    # 5. Sources (n√≠vel raiz)
    if data.get("sources") is None:
        data["sources"] = []
    
    # 6. Identity e Classification (objetos obrigat√≥rios - n√£o podem ser None)
    if data.get("identity") is None or not isinstance(data.get("identity"), dict):
        logger.warning("‚ö†Ô∏è identity √© None ou inv√°lido, criando objeto vazio")
        data["identity"] = {}
    if data.get("classification") is None or not isinstance(data.get("classification"), dict):
        logger.warning("‚ö†Ô∏è classification √© None ou inv√°lido, criando objeto vazio")
        data["classification"] = {}
    
    # 7. Team (garantir que √© objeto v√°lido)
    if data.get("team") is None or not isinstance(data.get("team"), dict):
        data["team"] = {}
    
    # 8. Contact (garantir que √© objeto v√°lido)
    if data.get("contact") is None or not isinstance(data.get("contact"), dict):
        data["contact"] = {}
    
    # 9. Reputation (garantir que √© objeto v√°lido)
    if data.get("reputation") is None or not isinstance(data.get("reputation"), dict):
        data["reputation"] = {}
    
    # 10. Offerings (garantir que √© objeto v√°lido)
    if data.get("offerings") is None or not isinstance(data.get("offerings"), dict):
        data["offerings"] = {}
    
    return data

async def _call_llm(client: AsyncOpenAI, model: str, text_content: str) -> CompanyProfile:
    """
    Faz a chamada real ao LLM com retry autom√°tico.
    Registra tempo total de infer√™ncia do modelo.
    """
    logger.info(f"üì§ Enviando requisi√ß√£o para {model} (tamanho do conte√∫do: {len(text_content)} chars)")
    start_ts = time.perf_counter()
    
    # Se o conte√∫do for muito pequeno, logar para debug
    if len(text_content) < 500:
        logger.warning(f"‚ö†Ô∏è Conte√∫do muito pequeno ({len(text_content)} chars) - pode indicar problema de scraping")
        # Extrair URL da p√°gina se presente
        if "--- PAGE START:" in text_content:
            url_line = text_content.split("\n")[0]
            logger.warning(f"üìÑ URL da p√°gina: {url_line.replace('--- PAGE START:', '').strip()}")
        # Mostrar primeiras linhas do conte√∫do
        preview = '\n'.join(text_content.split('\n')[:15])
        logger.warning(f"üìÑ Preview do conte√∫do ({len(text_content)} chars):\n{preview}")
    
    # Configura√ß√£o da requisi√ß√£o
    request_params = {
        "model": model,
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"Analise este conte√∫do e extraia os dados em Portugu√™s:\n\n{text_content}"}
        ],
        "temperature": 0.0,
        "response_format": {"type": "json_object"}
    }
    
    response = await client.chat.completions.create(**request_params)
    
    # Verificar se h√° conte√∫do na resposta
    if not response.choices or not response.choices[0].message.content:
        error_msg = f"‚ùå {model} retornou resposta vazia (sem choices ou content)"
        logger.error(error_msg)
        logger.error(f"üìä Response object: {response}")
        raise ValueError(error_msg)
    
    raw_content = response.choices[0].message.content.strip()
    duration = time.perf_counter() - start_ts
    logger.info(
        f"[PERF] llm step=model_inference model={model} "
        f"duration={duration:.3f}s response_chars={len(raw_content)}"
    )
    
    # Verificar se conte√∫do est√° realmente vazio
    if not raw_content or len(raw_content) < 10:
        error_msg = f"‚ùå {model} retornou conte√∫do vazio ou muito curto ({len(raw_content)} chars)"
        logger.error(error_msg)
        logger.error(f"üìÑ Conte√∫do recebido: '{raw_content}'")
        logger.error(f"üìä Response completo: {response}")
        raise ValueError(error_msg)
    
    logger.debug(f"üì• Resposta recebida de {model} (tamanho: {len(raw_content)} chars)")
    logger.debug(f"üìÑ Primeiros 200 chars da resposta: {raw_content[:200]}")
    
    # Limpar markdown se presente
    if raw_content.startswith("```json"):
        raw_content = raw_content[7:]
        logger.debug("üßπ Removido prefixo ```json")
    if raw_content.startswith("```"):
        raw_content = raw_content[3:]
        logger.debug("üßπ Removido prefixo ```")
    if raw_content.endswith("```"):
        raw_content = raw_content[:-3]
        logger.debug("üßπ Removido sufixo ```")
    
    # Parse JSON
    try:
        data = json.loads(raw_content)
        
        # Validar tipo antes de processar
        if isinstance(data, list):
            logger.warning(f"‚ö†Ô∏è JSON parseado √© uma lista, n√£o um objeto. Primeiro item ser√° usado.")
            if len(data) > 0 and isinstance(data[0], dict):
                data = data[0]
                logger.info("‚úÖ Primeiro item da lista extra√≠do como objeto")
            else:
                raise ValueError(f"Lista vazia ou primeiro item n√£o √© um dict. Tipo: {type(data[0]) if len(data) > 0 else 'N/A'}")
        
        if not isinstance(data, dict):
            raise ValueError(f"JSON parseado n√£o √© um dict. Tipo: {type(data)}")
        
        logger.debug(f"‚úÖ JSON parseado com sucesso. Chaves principais: {list(data.keys())}")
        
        # Verificar se h√° dados extra√≠dos
        has_data = False
        for key, value in data.items():
            if value and (isinstance(value, dict) and any(v for v in value.values() if v) or isinstance(value, list) and len(value) > 0):
                has_data = True
                logger.info(f"üìä Dados encontrados em '{key}': {len(value) if isinstance(value, list) else 'objeto'}")
                break
        
        # Verificar especificamente product_categories e seus items
        if "offerings" in data and isinstance(data["offerings"], dict):
            if "product_categories" in data["offerings"]:
                categories = data["offerings"]["product_categories"]
                total_categories = len(categories) if isinstance(categories, list) else 0
                categories_with_items = 0
                total_items = 0
                if isinstance(categories, list):
                    for cat in categories:
                        if isinstance(cat, dict) and cat.get("items"):
                            items = cat.get("items", [])
                            if isinstance(items, list) and len(items) > 0:
                                categories_with_items += 1
                                total_items += len(items)
                logger.info(f"üì¶ Product Categories: {total_categories} categorias, {categories_with_items} com items ({total_items} items totais)")
                if categories_with_items < total_categories:
                    empty_cats = [cat.get("category_name", "?") for cat in categories if isinstance(cat, dict) and not cat.get("items")]
                    logger.warning(f"‚ö†Ô∏è {total_categories - categories_with_items} categorias SEM items: {empty_cats[:5]}")
        
        if not has_data:
            logger.warning(f"‚ö†Ô∏è Resposta do {model} n√£o cont√©m dados extra√≠dos (todos os campos est√£o vazios)")
            logger.warning(f"üìã Estrutura completa recebida: {json.dumps(data, indent=2, ensure_ascii=False)[:1000]}")
        
        data = normalize_llm_response(data)
        logger.debug("‚úÖ Dados normalizados com sucesso")
        
        # Valida√ß√£o adicional antes de criar CompanyProfile
        # Verificar se campos obrigat√≥rios s√£o objetos v√°lidos (n√£o None)
        if data.get("identity") is None or not isinstance(data.get("identity"), dict):
            logger.error(f"‚ùå identity inv√°lido ap√≥s normaliza√ß√£o: {type(data.get('identity'))}")
            raise ValueError(f"identity deve ser um dict, recebido: {type(data.get('identity'))}")
        if data.get("classification") is None or not isinstance(data.get("classification"), dict):
            logger.error(f"‚ùå classification inv√°lido ap√≥s normaliza√ß√£o: {type(data.get('classification'))}")
            raise ValueError(f"classification deve ser um dict, recebido: {type(data.get('classification'))}")
        
        try:
            profile = CompanyProfile(**data)
            logger.info(f"‚úÖ CompanyProfile criado com sucesso a partir de {model}")
            return profile
        except Exception as e:
            logger.error(f"‚ùå Erro ao criar CompanyProfile de {model}: {e}")
            logger.error(f"üìã Dados problem√°ticos: {json.dumps(data, indent=2, ensure_ascii=False)[:1000]}")
            raise e
        
    except json.JSONDecodeError as e:
        logger.warning(f"‚ö†Ô∏è JSON padr√£o falhou para {model}. Tentando reparar JSON malformado...")
        logger.debug(f"‚ùå Erro de JSON: {e}")
        logger.debug(f"üìÑ Conte√∫do problem√°tico (primeiros 500 chars): {raw_content[:500]}")
        try:
            data = json_repair.loads(raw_content)
            logger.info("‚úÖ JSON reparado com sucesso")
            
            # Validar tipo ap√≥s reparo
            if isinstance(data, list):
                logger.warning(f"‚ö†Ô∏è JSON reparado ainda √© uma lista. Primeiro item ser√° usado.")
                if len(data) > 0 and isinstance(data[0], dict):
                    data = data[0]
                    logger.info("‚úÖ Primeiro item da lista extra√≠do ap√≥s reparo")
                else:
                    raise ValueError(f"Lista vazia ou inv√°lida ap√≥s reparo")
            
            if not isinstance(data, dict):
                raise ValueError(f"JSON reparado n√£o √© um dict. Tipo: {type(data)}")
            
            data = normalize_llm_response(data)
            profile = CompanyProfile(**data)
            return profile
        except Exception as e2:
            logger.error(f"‚ùå Falha cr√≠tica no parse do JSON mesmo ap√≥s reparo: {e2}")
            logger.error(f"üìÑ Conte√∫do problem√°tico (primeiros 500 chars): {raw_content[:500]}")
            raise e2
    except Exception as e:
        logger.error(f"‚ùå Erro ao validar/construir CompanyProfile de {model}: {e}")
        logger.error(f"üìä Tipo de dados recebido: {type(data)}")
        logger.error(f"üìÑ Dados recebidos: {str(data)[:500]}")
        raise e

async def analyze_content_with_fallback(text_content: str, provider_name: Optional[str] = None) -> CompanyProfile:
    """
    Tenta analisar o conte√∫do com fallback autom√°tico entre provedores.
    Se um provedor falhar com RateLimitError, tenta o pr√≥ximo.
    Usa semaphores individuais por provedor para respeitar rate limits.
    Registra tempo total da an√°lise considerando fallback.
    """
    start_ts = time.perf_counter()
    # Throttling global
    async with llm_global_semaphore:
        # Se um provedor espec√≠fico foi solicitado, tentar apenas ele
        if provider_name:
            providers_to_try = [p for p in AVAILABLE_PROVIDERS if p[0] == provider_name]
        else:
            providers_to_try = AVAILABLE_PROVIDERS
        
        last_error = None
        
        for name, key, base_url, model in providers_to_try:
            # Throttling espec√≠fico do provedor
            provider_semaphore = llm_semaphores.get(name, asyncio.Semaphore(3))  # Default: 3
            
            async with provider_semaphore:
                try:
                    logger.info(f"Tentando an√°lise com {name} ({model})...")
                    
                    # Criar cliente para este provedor
                    client_args = {"api_key": key, "base_url": base_url}
                    provider_client = AsyncOpenAI(**client_args)
                    
                    # Tentar an√°lise
                    profile = await _call_llm(provider_client, model, text_content)
                    logger.info(f"‚úÖ An√°lise bem-sucedida com {name}")
                    return profile
                    
                except RateLimitError as e:
                    logger.warning(f"‚ö†Ô∏è {name} rate limited: {e}")
                    last_error = e
                    continue  # Tentar pr√≥ximo provedor
                    
                except BadRequestError as e:
                    logger.error(f"‚ùå {name} bad request (provavelmente conte√∫do muito grande): {e}")
                    last_error = e
                    # N√£o tentar outros provedores para BadRequest, propagar o erro
                    raise e
                    
                except Exception as e:
                    logger.error(f"‚ùå {name} falhou com erro inesperado: {e}")
                    last_error = e
                    continue  # Tentar pr√≥ximo provedor
        
        # Se chegou aqui, todos falharam
        total_duration = time.perf_counter() - start_ts
        error_msg = f"Todos os provedores LLM falharam. √öltimo erro: {last_error}"
        logger.error(f"[PERF] llm step=analyze_content_with_fallback_all_failed duration={total_duration:.3f}s")
        logger.error(error_msg)
        raise Exception(error_msg)

async def process_chunk_with_retry(chunk: str, chunk_num: int, total_chunks: int, primary_provider: Optional[str] = None) -> Optional[CompanyProfile]:
    """
    Processa um chunk (geralmente uma p√°gina) com retry e fallback.
    Adiciona logs detalhados para rastrear extra√ß√£o de categorias.
    
    Estrat√©gia:
    1. Tenta com o provedor prim√°rio designado
    2. Se falhar, tenta com outros provedores dispon√≠veis
    3. Se todos falharem, tenta reprocessar uma vez
    4. Se ainda falhar, retorna None
    """
    # Extrair URL da p√°gina do chunk para logging
    page_url = "desconhecida"
    if "--- PAGE START:" in chunk:
        try:
            first_line = chunk.split("\n")[0]
            if "--- PAGE START:" in first_line:
                page_url = first_line.replace("--- PAGE START:", "").strip()
        except:
            pass
    
    logger.info(f"üìÑ Processando Chunk {chunk_num}/{total_chunks} (P√°gina: {page_url[:80]}...)")
    # Lista de provedores para tentar (come√ßando com o prim√°rio)
    providers_to_try = []
    if primary_provider:
        # Adicionar provedor prim√°rio primeiro
        primary = [p for p in AVAILABLE_PROVIDERS if p[0] == primary_provider]
        if primary:
            providers_to_try.append(primary[0])
        # Adicionar outros provedores como fallback
        for p in AVAILABLE_PROVIDERS:
            if p[0] != primary_provider:
                providers_to_try.append(p)
    else:
        # Sem provedor prim√°rio, tentar todos em ordem
        providers_to_try = list(AVAILABLE_PROVIDERS)
    
    # Log do conte√∫do do chunk se for muito pequeno (pode indicar problema de scraping)
    chunk_size = len(chunk)
    if chunk_size < 500:
        logger.warning(f"‚ö†Ô∏è Chunk {chunk_num}/{total_chunks} tem apenas {chunk_size} chars - pode ter pouco conte√∫do para extrair")
        # Mostrar primeiras linhas do conte√∫do para debug
        first_lines = '\n'.join(chunk.split('\n')[:10])
        logger.debug(f"üìÑ Primeiras 10 linhas do chunk {chunk_num}: {first_lines[:500]}")
    
    # Primeira tentativa: tentar todos os provedores
    last_error = None
    for name, key, base_url, model in providers_to_try:
        try:
            logger.info(f"üîÑ Chunk {chunk_num}/{total_chunks}: Tentando com {name} ({model})...")
            profile = await analyze_content_with_fallback(chunk, provider_name=name)
            
            # Log detalhado das categorias extra√≠das deste chunk
            if profile and hasattr(profile, 'offerings') and profile.offerings:
                categories = profile.offerings.product_categories if hasattr(profile.offerings, 'product_categories') else []
                if categories:
                    cat_names = [cat.category_name for cat in categories if hasattr(cat, 'category_name') and cat.category_name]
                    total_items = sum(len(cat.items) if hasattr(cat, 'items') and cat.items else 0 for cat in categories)
                    logger.info(f"‚úÖ Chunk {chunk_num}/{total_chunks}: Sucesso com {name} - Extra√≠das {len(cat_names)} categorias ({total_items} items totais): {', '.join(cat_names[:5])}{'...' if len(cat_names) > 5 else ''}")
                else:
                    logger.warning(f"‚ö†Ô∏è Chunk {chunk_num}/{total_chunks}: Sucesso com {name} mas NENHUMA categoria extra√≠da (conte√∫do pode estar vazio ou incompleto)")
                    # Se n√£o extraiu categorias e o chunk √© pequeno, logar o conte√∫do completo
                    if chunk_size < 1000:
                        logger.debug(f"üìÑ Conte√∫do completo do chunk {chunk_num} (para debug):\n{chunk[:2000]}")
            else:
                logger.warning(f"‚ö†Ô∏è Chunk {chunk_num}/{total_chunks}: Sucesso com {name} mas perfil vazio ou sem offerings")
            
            return profile
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Chunk {chunk_num}/{total_chunks}: {name} falhou: {type(e).__name__}")
            last_error = e
            continue  # Tentar pr√≥ximo provedor
    
    # Se todos os provedores falharam, tentar reprocessar uma vez (retry)
    logger.warning(f"üîÑ Chunk {chunk_num}/{total_chunks}: Todos os provedores falharam. Tentando reprocessar uma vez...")
    for name, key, base_url, model in providers_to_try:
        try:
            logger.info(f"üîÑ Chunk {chunk_num}/{total_chunks}: Retry com {name} ({model})...")
            profile = await analyze_content_with_fallback(chunk, provider_name=name)
            logger.info(f"‚úÖ Chunk {chunk_num}/{total_chunks}: Sucesso no retry com {name}")
            return profile
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Chunk {chunk_num}/{total_chunks}: Retry com {name} falhou: {type(e).__name__}")
            last_error = e
            continue
    
    # Se ainda falhou ap√≥s retry, retornar None
    logger.error(f"‚ùå Chunk {chunk_num}/{total_chunks}: Falhou ap√≥s tentar todos os provedores e retry. √öltimo erro: {last_error}")
    return None

async def analyze_content(text_content: str) -> CompanyProfile:
    """
    Fun√ß√£o principal de an√°lise com chunking autom√°tico e consolida√ß√£o.
    SEMPRE processa uma p√°gina por requisi√ß√£o LLM para garantir que todas as p√°ginas sejam analisadas.
    Distribui chunks entre m√∫ltiplos provedores LLM para evitar rate limits.
    Registra m√©tricas de tempo de chunking, processamento de chunks e merge final.
    """
    global_start = time.perf_counter()
    tokens = estimate_tokens(text_content)
    logger.info(f"Conte√∫do total: ~{tokens:,} tokens estimados")
    
    # SEMPRE dividir por p√°ginas (uma p√°gina por requisi√ß√£o LLM)
    # Isso garante que todas as p√°ginas sejam analisadas, mesmo que o conte√∫do total seja pequeno
    MAX_TOKENS = 500_000  # 50% do limite do Gemini 2.0 Flash (1.048.575) - Muito conservador
    
    chunk_start = time.perf_counter()
    logger.info("Aplicando chunking por p√°gina (uma p√°gina por requisi√ß√£o LLM)...")
    chunks = chunk_content(text_content, MAX_TOKENS)
    chunk_duration = time.perf_counter() - chunk_start
    logger.info(
        f"[PERF] llm step=chunk_content chunks={len(chunks)} "
        f"duration={chunk_duration:.3f}s estimated_tokens={tokens}"
    )
    
    # Se houver apenas 1 chunk e for pequeno, ainda assim processar normalmente
    if len(chunks) == 1:
        logger.info(f"Uma √∫nica p√°gina detectada, processando diretamente...")
        single_start = time.perf_counter()
        profile = await analyze_content_with_fallback(chunks[0])
        total_duration = time.perf_counter() - global_start
        logger.info(
            f"[PERF] llm step=analyze_content_single_chunk duration={total_duration:.3f}s"
        )
        return profile
    
    # LOAD BALANCING: Distribuir chunks entre provedores dispon√≠veis em round-robin
    if len(AVAILABLE_PROVIDERS) > 1:
        logger.info(f"üîÑ Distribuindo {len(chunks)} chunks entre {len(AVAILABLE_PROVIDERS)} provedores LLM:")
        for provider_name, _, _, _ in AVAILABLE_PROVIDERS:
            logger.info(f"  ‚Ä¢ {provider_name}")
        
        # Atribuir cada chunk a um provedor em round-robin e processar com retry
        tasks = []
        for i, chunk in enumerate(chunks):
            provider_idx = i % len(AVAILABLE_PROVIDERS)
            provider_name = AVAILABLE_PROVIDERS[provider_idx][0]
            logger.info(f"  Chunk {i+1}/{len(chunks)} ‚Üí {provider_name} (com retry e fallback)")
            tasks.append(process_chunk_with_retry(chunk, i+1, len(chunks), primary_provider=provider_name))
    else:
        # Apenas 1 provedor dispon√≠vel, usar retry mesmo assim
        logger.info(f"Processando {len(chunks)} chunks com provedor √∫nico (com retry)...")
        tasks = [process_chunk_with_retry(chunk, i+1, len(chunks), primary_provider=None) 
                for i, chunk in enumerate(chunks)]
    
    # Processar todos os chunks em paralelo (com throttling do semaphore)
    process_chunks_start = time.perf_counter()
    partial_profiles = await asyncio.gather(*tasks, return_exceptions=True)
    
    # Filtrar exce√ß√µes e None, manter apenas perfis v√°lidos
    valid_profiles = []
    failed_chunks = []
    for i, result in enumerate(partial_profiles):
        if isinstance(result, Exception):
            logger.error(f"‚ùå Chunk {i+1}/{len(chunks)} falhou com exce√ß√£o: {result}")
            failed_chunks.append(i+1)
        elif result is None:
            logger.warning(f"‚ö†Ô∏è Chunk {i+1}/{len(chunks)} retornou None (falhou ap√≥s todos os retries)")
            failed_chunks.append(i+1)
        else:
            # Verificar se o perfil tem dados
            if isinstance(result, CompanyProfile):
                p_dict = result.model_dump()
                filled_fields = sum(1 for k, v in p_dict.items() 
                                  if v and (isinstance(v, dict) and any(v.values()) or isinstance(v, list) and len(v) > 0))
                if filled_fields > 0:
                    logger.info(f"‚úÖ Chunk {i+1}/{len(chunks)} processado com sucesso ({filled_fields} campos preenchidos)")
                else:
                    logger.warning(f"‚ö†Ô∏è Chunk {i+1}/{len(chunks)} processado mas sem dados extra√≠dos")
            valid_profiles.append(result)
    
    if failed_chunks:
        logger.warning(f"‚ö†Ô∏è {len(failed_chunks)} chunks falharam ap√≥s retries: {failed_chunks}")
    
    if not valid_profiles:
        total_duration = time.perf_counter() - global_start
        logger.error(
            f"[PERF] llm step=analyze_content_all_chunks_failed duration={total_duration:.3f}s"
        )
        raise Exception("Todos os chunks falharam no processamento")
    
    # Analisar perfis antes do merge
    process_chunks_duration = time.perf_counter() - process_chunks_start
    logger.info(
        f"[PERF] llm step=process_chunks valid_profiles={len(valid_profiles)} "
        f"total_chunks={len(chunks)} duration={process_chunks_duration:.3f}s"
    )
    logger.info(f"üìä An√°lise pr√©-merge: {len(valid_profiles)}/{len(chunks)} perfis v√°lidos")
    total_filled = sum(1 for p in valid_profiles 
                    if isinstance(p, CompanyProfile) and 
                    any(v for v in p.model_dump().values() 
                        if v and (isinstance(v, dict) and any(v.values()) or isinstance(v, list) and len(v) > 0)))
    logger.info(f"üìä Perfis com dados: {total_filled}/{len(valid_profiles)}")
    
    # Consolidar resultados
    merge_start = time.perf_counter()
    logger.info(f"‚úÖ Consolidando {len(valid_profiles)}/{len(chunks)} perfis parciais bem-sucedidos...")
    final_profile = merge_profiles(valid_profiles)
    merge_duration = time.perf_counter() - merge_start
    logger.info(
        f"[PERF] llm step=merge_profiles duration={merge_duration:.3f}s "
        f"profiles_input={len(valid_profiles)}"
    )
    
    # Verificar resultado final
    final_dict = final_profile.model_dump()
    final_filled = sum(1 for k, v in final_dict.items() 
                      if v and (isinstance(v, dict) and any(v.values()) or isinstance(v, list) and len(v) > 0))
    total_duration = time.perf_counter() - global_start
    logger.info(f"üìä Resultado final: {final_filled} campos preenchidos")
    logger.info(
        f"[PERF] llm step=analyze_content_total duration={total_duration:.3f}s "
        f"chunks={len(chunks)}"
    )
    
    return final_profile
