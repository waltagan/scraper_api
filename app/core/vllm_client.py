"""
Cliente ass√≠ncrono para o modelo LLM hospedado no RunPod via SGLang/vLLM.
Usa a biblioteca openai com AsyncOpenAI para compatibilidade total.

v4.0: Suporte a Structured Output via SGLang/XGrammar
      - response_format com json_schema para outputs estruturados
      - Compat√≠vel com SGLang (XGrammar) e vLLM

v9.1: Suporte a SGLang sem autentica√ß√£o
      - SEMPRE usa httpx diretamente (nunca AsyncOpenAI)
      - NUNCA envia Authorization header (SGLang rejeita qualquer auth)
"""
import logging
import time
from typing import Optional, List, Dict, Any
import httpx
from openai import AsyncOpenAI

from app.core.config import settings

logger = logging.getLogger(__name__)

# Singleton do cliente ass√≠ncrono
_vllm_client: Optional[AsyncOpenAI] = None


def get_vllm_client() -> AsyncOpenAI:
    """
    Retorna cliente SGLang ass√≠ncrono (singleton).
    
    IMPORTANTE: Apesar do nome "vllm_client", este cliente conecta ao SGLang.
    O nome foi mantido por compatibilidade com c√≥digo existente.

    NOTA: Este cliente n√£o √© mais usado diretamente. Todas as chamadas ao SGLang
    usam httpx diretamente (sem Authorization header) na fun√ß√£o chat_completion().
    Este singleton √© mantido apenas por compatibilidade com c√≥digo legado.
    
    v9.1: SGLang SEMPRE usa httpx diretamente (nunca AsyncOpenAI)
          - NUNCA envia Authorization header (SGLang rejeita qualquer auth)
    
    Returns:
        AsyncOpenAI: Cliente ass√≠ncrono (n√£o usado para SGLang)
    """
    global _vllm_client
    if _vllm_client is None:
        _vllm_client = AsyncOpenAI(
            base_url=settings.VLLM_BASE_URL,
            api_key=settings.VLLM_API_KEY,
        )
        
        # Log com indica√ß√£o se est√° usando auth real ou dummy
        auth_status = (
            "dummy (sem auth)" if settings.VLLM_API_KEY == "dummy" 
            else "***" if settings.VLLM_API_KEY 
            else "NONE"
        )
        
        logger.info(
            f"‚úÖ Cliente SGLang criado: base_url={settings.VLLM_BASE_URL}, "
            f"model={settings.VLLM_MODEL}, api_key={auth_status}"
        )
    return _vllm_client


async def chat_completion(
    messages: List[Dict[str, str]],
    temperature: float = 0.7,
    max_tokens: int = 500,
    model: Optional[str] = None,
    response_format: Optional[Dict[str, Any]] = None,
) -> Any:
    """
    Executa chat completion no modelo SGLang (ass√≠ncrono).
    
    IMPORTANTE: Sistema usa APENAS SGLang (n√£o vLLM).
    
    v4.0: Suporte a Structured Output
          - response_format com json_schema para outputs estruturados
          - SGLang usa XGrammar para garantir JSON v√°lido
    
    Args:
        messages: Lista de mensagens no formato OpenAI
            Exemplo: [
                {"role": "system", "content": "Voc√™ √© um assistente √∫til."},
                {"role": "user", "content": "Qual √© o site oficial da empresa X?"}
            ]
        temperature: Temperatura para gera√ß√£o (0.0 a 1.0)
        max_tokens: N√∫mero m√°ximo de tokens na resposta
        model: Modelo a usar (default: VLLM_MODEL do config)
        response_format: Formato de resposta estruturada (opcional)
            Exemplo json_schema: {
                "type": "json_schema",
                "json_schema": {"name": "output", "schema": {...}}
            }
            Exemplo json_object: {"type": "json_object"}
    
    Returns:
        Resposta completa do modelo no formato OpenAI
    
    Exemplo:
        response = await chat_completion([
            {"role": "system", "content": "Voc√™ √© um assistente √∫til."},
            {"role": "user", "content": "Qual √© o site oficial da empresa X?"}
        ])
        answer = response.choices[0].message.content
    """
    client = get_vllm_client()
    
    try:
        # Construir par√¢metros da requisi√ß√£o
        request_params = {
            "model": model or settings.VLLM_MODEL,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }
        
        # Adicionar response_format se fornecido (SGLang/XGrammar suporta)
        if response_format:
            request_params["response_format"] = response_format
            logger.debug(
                f"üéØ Structured output habilitado: type={response_format.get('type')}"
            )
        
        # v9.1: SEMPRE usar httpx diretamente para SGLang (NUNCA enviar Authorization header)
        # SGLang usa token via query parameter (?token=XXX) em vez de Authorization header
        # Construir URL com token se dispon√≠vel
        from urllib.parse import urlencode
        
        request_url = f"{settings.VLLM_BASE_URL}/chat/completions"
        
        # Adicionar token como query parameter se dispon√≠vel (n√£o √© "dummy")
        if settings.VLLM_API_KEY and settings.VLLM_API_KEY != "dummy":
            request_url += f"?token={settings.VLLM_API_KEY}"
            logger.debug(
                f"vllm_client: Usando httpx direto para SGLang com token via query parameter "
                f"(token={settings.VLLM_API_KEY[:20]}...)"
            )
        else:
            logger.debug(
                f"vllm_client: Usando httpx direto para SGLang (sem token)"
            )
        
        async with httpx.AsyncClient(timeout=120.0) as http_client:
            http_response = await http_client.post(
                request_url,
                json=request_params,
                headers={"Content-Type": "application/json"}
                # SEM Authorization header - SGLang usa token via query parameter
            )
            
            http_response.raise_for_status()
            response_data = http_response.json()
            
            # Converter resposta httpx para formato OpenAI-like
            from openai.types.chat import ChatCompletion, ChatCompletionMessage, Choice
            from openai.types.completion_usage import CompletionUsage
            
            # Extrair dados da resposta
            choices_data = response_data.get("choices", [])
            if not choices_data:
                raise ValueError("Resposta sem choices")
            
            message_data = choices_data[0].get("message", {})
            content = message_data.get("content", "")
            
            # Criar objeto de resposta compat√≠vel com OpenAI
            response = ChatCompletion(
                id=response_data.get("id", "unknown"),
                object="chat.completion",
                created=response_data.get("created", int(time.time())),
                model=response_data.get("model", model or settings.VLLM_MODEL),
                choices=[
                    Choice(
                        index=0,
                        message=ChatCompletionMessage(
                            role=message_data.get("role", "assistant"),
                            content=content
                        ),
                        finish_reason=choices_data[0].get("finish_reason", "stop")
                    )
                ],
                usage=CompletionUsage(
                    prompt_tokens=response_data.get("usage", {}).get("prompt_tokens", 0),
                    completion_tokens=response_data.get("usage", {}).get("completion_tokens", 0),
                    total_tokens=response_data.get("usage", {}).get("total_tokens", 0)
                ) if "usage" in response_data else None
            )
        
        # Log de uso de tokens
        if response.usage:
            logger.debug(
                f"üî¢ Tokens: prompt={response.usage.prompt_tokens}, "
                f"completion={response.usage.completion_tokens}, "
                f"total={response.usage.total_tokens}"
            )
        
        return response
    except Exception as e:
        logger.error(f"‚ùå Erro ao chamar SGLang: {e}")
        raise


async def check_vllm_health() -> Dict[str, Any]:
    """
    Verifica se o servidor SGLang est√° respondendo.
    
    IMPORTANTE: Apesar do nome "check_vllm_health", verifica SGLang.
    Nome mantido por compatibilidade.
    
    v9.1: Health check mais robusto
          - Tenta /health primeiro
          - Se falhar, faz chamada de teste ao modelo
          - Retorna mais detalhes sobre erros
    
    Returns:
        Dict com status e lat√™ncia
    
    Exemplo de retorno:
        {
            "status": "healthy",
            "latency_ms": 45.2,
            "model": "Qwen/Qwen3-8B",
            "endpoint": "https://example.trycloudflare.com/v1"
        }
    """
    start = time.perf_counter()
    
    try:
        async with httpx.AsyncClient(timeout=10.0) as http_client:
            # Tentar endpoint /health primeiro
            health_url = settings.VLLM_BASE_URL.replace('/v1', '') + '/health'
            
            try:
                response = await http_client.get(health_url)
                latency_ms = (time.perf_counter() - start) * 1000
                
                # SGLang/vLLM retorna 200 quando saud√°vel
                if response.status_code == 200:
                    logger.debug(f"‚úÖ SGLang /health endpoint OK: {response.status_code}")
                    return {
                        "status": "healthy",
                        "latency_ms": round(latency_ms, 2),
                        "model": settings.VLLM_MODEL,
                        "endpoint": settings.VLLM_BASE_URL,
                        "health_endpoint": "OK"
                    }
                else:
                    # Status != 200, mas endpoint existe - tentar chamada de teste
                    logger.warning(
                        f"‚ö†Ô∏è SGLang /health retornou {response.status_code}, "
                        f"tentando chamada de teste ao modelo..."
                    )
                    raise httpx.RequestError("Health endpoint returned non-200")
                    
            except (httpx.RequestError, httpx.HTTPError, Exception) as health_error:
                # Se /health n√£o existir ou falhar, tentar uma chamada m√≠nima ao modelo
                logger.debug(
                    f"‚ÑπÔ∏è Endpoint /health n√£o dispon√≠vel ({type(health_error).__name__}), "
                    f"tentando chamada de teste ao modelo..."
                )
                
                try:
                    # Reset timer para medir lat√™ncia da chamada ao modelo
                    model_start = time.perf_counter()
                    
                    test_response = await chat_completion(
                        messages=[{"role": "user", "content": "test"}],
                        max_tokens=5,
                        temperature=0.0,
                    )
                    
                    model_latency_ms = (time.perf_counter() - model_start) * 1000
                    
                    # Se chegou aqui, modelo est√° respondendo
                    logger.info(
                        f"‚úÖ SGLang modelo respondeu em {model_latency_ms:.0f}ms "
                        f"(endpoint /health n√£o dispon√≠vel, mas modelo OK)"
                    )
                    
                    return {
                        "status": "healthy",
                        "latency_ms": round(model_latency_ms, 2),
                        "model": settings.VLLM_MODEL,
                        "endpoint": settings.VLLM_BASE_URL,
                        "health_endpoint": "unavailable",
                        "health_method": "model_test"
                    }
                    
                except Exception as model_error:
                    # Falhou tanto /health quanto chamada ao modelo
                    total_latency_ms = (time.perf_counter() - start) * 1000
                    error_msg = str(model_error)
                    
                    logger.error(
                        f"‚ùå SGLang n√£o respondeu: {type(model_error).__name__}: {error_msg}"
                    )
                    
                    return {
                        "status": "unhealthy",
                        "error": error_msg,
                        "error_type": type(model_error).__name__,
                        "latency_ms": round(total_latency_ms, 2),
                        "model": settings.VLLM_MODEL,
                        "endpoint": settings.VLLM_BASE_URL,
                    }
                    
    except Exception as e:
        # Erro geral (ex: timeout no httpx.AsyncClient)
        latency_ms = (time.perf_counter() - start) * 1000
        error_msg = str(e)
        
        logger.error(f"‚ùå Erro cr√≠tico no health check: {type(e).__name__}: {error_msg}")
        
        return {
            "status": "error",
            "error": error_msg,
            "error_type": type(e).__name__,
            "latency_ms": round(latency_ms, 2),
            "model": settings.VLLM_MODEL,
            "endpoint": settings.VLLM_BASE_URL,
        }

