import asyncio
import logging
import time
import uuid
from typing import Optional, List
from fastapi import FastAPI, HTTPException, Request, Depends
from fastapi.responses import JSONResponse
from pydantic import BaseModel, HttpUrl

# Configurar encoding UTF-8 para Windows
import sys
if sys.platform == 'win32':
    import io
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass  # Se j√° estiver configurado, ignora

# Carregar vari√°veis de ambiente do arquivo .env
try:
    from dotenv import load_dotenv
    load_dotenv()
    print("[OK] Arquivo .env carregado")
except ImportError:
    print("[WARN] python-dotenv nao instalado - usando variaveis de ambiente do sistema")

from app.schemas.profile import CompanyProfile
from app.services.scraper import scrape_url
from app.services.profile_builder import analyze_content
from app.services.discovery import find_company_website
from app.core.security import get_api_key
from app.core.logging_utils import setup_logging
from app.services.llm_manager import start_health_monitor
from app.core.database import get_pool, close_pool, test_connection
from app.core.vllm_client import check_vllm_health
from app.api.v2.router import router as v2_router

# Configurar Logging (JSON Structured)
setup_logging()
logger = logging.getLogger(__name__)

app = FastAPI(title="B2B Flash Profiler")

# Registrar router v2
app.include_router(v2_router, prefix="/v2")

# Iniciar monitor de sa√∫de dos providers LLM no startup
@app.on_event("startup")
async def startup_event():
    """Executado quando a aplica√ß√£o inicia"""
    # Inicializar pool de conex√µes do banco de dados
    try:
        await get_pool()
        # Testar conex√£o
        if await test_connection():
            logger.info("‚úÖ Conex√£o com banco de dados estabelecida")
        else:
            logger.warning("‚ö†Ô∏è Falha ao testar conex√£o com banco de dados")
    except Exception as e:
        logger.error(f"‚ùå Erro ao inicializar banco de dados: {e}")
    
    # Health check do vLLM
    try:
        vllm_health = await check_vllm_health()
        logger.info(f"üîç vLLM Health: {vllm_health}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erro ao verificar sa√∫de do vLLM: {e}")
    
    start_health_monitor()

    logger.info("üöÄ Aplica√ß√£o inicializada com sucesso")


@app.on_event("shutdown")
async def shutdown_event():
    """Executado quando a aplica√ß√£o encerra"""
    await close_pool()
    logger.info("üîå Aplica√ß√£o encerrada")


class CompanyRequest(BaseModel):
    url: Optional[HttpUrl] = None
    razao_social: Optional[str] = None
    nome_fantasia: Optional[str] = None
    cnpj: Optional[str] = None
    # Novos campos para discovery aprimorado
    email: Optional[str] = None
    municipio: Optional[str] = None
    cnaes: Optional[List[str]] = None

# --- Global Exception Handlers ---

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Global Error: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal Server Error", "error": str(exc)}
    )

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail}
    )




# --- Main Analysis Endpoint ---

@app.post("/monta_perfil", response_model=CompanyProfile, dependencies=[Depends(get_api_key)])
async def monta_perfil(request: CompanyRequest):
    """
    Monta o perfil completo de uma empresa. 
    Aceita URL diretamente OU dados da empresa (razao_social, nome_fantasia, cnpj) para buscar o site automaticamente.
    Aplica timeout de 300 segundos.
    """
    start_ts = time.perf_counter()
    url_str = str(request.url) if request.url else None
    
    # Gerar ID √∫nico para rastreamento
    request_id = str(uuid.uuid4())[:8]
    
    # Identificador de contexto para logs
    ctx_label = f"[{request_id}][CNPJ: {request.cnpj or 'N/A'} - {request.nome_fantasia or request.razao_social or 'Unknown'}]"
    

    try:
        # Discovery Phase
        if not url_str:
            if not any([request.razao_social, request.nome_fantasia, request.cnpj]):
                raise HTTPException(
                    status_code=400,
                    detail="Deve fornecer URL ou dados da empresa (razao_social, nome_fantasia, cnpj)"
                )
            
            
            try:
                # Aplicar timeout do Discovery (70s da config)
                from app.services.discovery.discovery_service import DISCOVERY_TIMEOUT
                try:
                    found_url = await asyncio.wait_for(
                        find_company_website(
                    request.razao_social or "",
                    request.nome_fantasia or "",
                    request.cnpj or "",
                    email=request.email,
                    municipio=request.municipio,
                    cnaes=request.cnaes,
                    ctx_label=ctx_label,
                    request_id=request_id
                        ),
                        timeout=DISCOVERY_TIMEOUT
                    )
                except asyncio.TimeoutError:
                    error_msg = f"Timeout ap√≥s {DISCOVERY_TIMEOUT}s"
                    logger.error(f"{ctx_label}[DISCOVERY] {error_msg}")
                    raise HTTPException(status_code=504, detail=f"Discovery timeout ap√≥s {DISCOVERY_TIMEOUT}s")
            except HTTPException:
                raise
            except Exception as e:
                raise

            if not found_url:
                raise HTTPException(
                    status_code=404,
                    detail="Site oficial n√£o encontrado com os dados fornecidos."
                )

            url_str = found_url

        # Wrap the orchestration in a task to enforce timeout
        result = await asyncio.wait_for(
            process_analysis(url_str, ctx_label, request_id), 
            timeout=300.0
        )
        
        # Add discovery source metadata if discovered
        if not request.url and url_str:
            if not result.sources:
                result.sources = []
            result.sources.insert(0, f"Discovered via Google Search: {url_str}")
            
        # Track completion (ass√≠ncrono, n√£o bloqueante)
        total = time.perf_counter() - start_ts
        
        logger.info(f"{ctx_label} [PERF] monta_perfil end url={url_str} total={total:.3f}s")
        return result
        
    except asyncio.TimeoutError:
        total = time.perf_counter() - start_ts
        logger.error(f"{ctx_label} [PERF] monta_perfil timeout url={url_str} total={total:.3f}s")
        raise HTTPException(status_code=504, detail="Analysis timed out (exceeded 300s)")
        
    except Exception as e:
        total = time.perf_counter() - start_ts
        
        # If it's already an HTTPException, handle accordingly
        if isinstance(e, HTTPException):
            if e.status_code < 500:
                logger.warning(f"{ctx_label} [PERF] monta_perfil finished with expected error url={url_str} total={total:.3f}s code={e.status_code} detail={e.detail}")
            else:
                logger.error(f"{ctx_label} [PERF] monta_perfil failed with HTTP error url={url_str} total={total:.3f}s code={e.status_code} detail={e.detail}")
            raise e
            
        logger.error(f"{ctx_label} [PERF] monta_perfil failed url={url_str} total={total:.3f}s error={e}")
        raise HTTPException(status_code=500, detail=str(e))


async def process_analysis(url: str, ctx_label: str = "", request_id: str = "") -> CompanyProfile:
    """
    Orquestra o fluxo principal de an√°lise.
    
    v2.0: M√≥dulo de documentos (PDF/DOC) removido para simplificar fluxo.
    """
    # 1. Scrape the main website AND subpages
    markdown, _, scraped_urls = await scrape_url(url, max_subpages=50, ctx_label=ctx_label, request_id=request_id)
    if not markdown:
        raise Exception("Failed to scrape content from the URL")
    
    # 2. Prepare content for LLM
    combined_text = f"--- WEB CRAWL START ({url}) ---\n{markdown}\n--- WEB CRAWL END ---\n\n"

    # 3. LLM Analysis
    # Extrair informa√ß√µes do ctx_label para debug
    import re
    cnpj_match = re.search(r'CNPJ: ([^-]+)', ctx_label) if ctx_label else None
    name_match = re.search(r'CNPJ: [^-]+ - (.+)\]', ctx_label) if ctx_label else None
    extracted_cnpj = cnpj_match.group(1).strip() if cnpj_match else None
    extracted_name = name_match.group(1).strip() if name_match else None
    
    profile = await analyze_content(
        combined_text, 
        ctx_label=ctx_label, 
        request_id=request_id,
        url=url,
        cnpj=extracted_cnpj,
        company_name=extracted_name
    )
    
    # 4. Add Sources
    profile.sources = list(set(scraped_urls))
    
    return profile


@app.get("/")
async def root():
    return {"status": "ok", "service": "B2B Flash Profiler"}
